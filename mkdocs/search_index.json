{
    "docs": [
        {
            "location": "/", 
            "text": "DataRescue Workflow -- Overview\n\n\nThis document describes the workflow we use for Data Rescue activites as developed by the \nDataRefuge project\n and \nEDGI\n, both at in-person events and when people work remotely. It explains the process that a url/dataset goes through from the time it has been identified, either by a \nseeder \n sorter\n as \"uncrawlable,\" or by other means, until it is made available as a record in the \ndatarefuge.org\n ckan data catalog. The process involves several stages, and is designed to maximize smooth hand-offs so that each phase is handled by someone with distinct expertise in the area they're tackling, while the data is always being tracked for security.\n\n\nBefore you begin\n\n\nWe are so glad that you are participating in this project!\n\n\n\n\n\nIf you are an Event Organizer\n:\n- Learn about what you need to do to prepare the event \nhere\n.\n\n\nIf you are a regular participant\n:\n- Get a role assignment (e.g., Seeder, or Harvester), get account credentials needed for your role, and make sure you have access to the key documents and app needed to do the work. The Event/Remote organizers will tell you how proceed to do all this.\n- Go over the workflow documentation below, in particular the pages corresponding to your role.\n\n\n\n\nPlan Overview\n\n\n1. \nSeeders/Sorters\n\n\nSeeders and Sorters canvass the resources of a given government agency, identifying important URLs. They identify whether those URLs can be crawled by the Internet Archive's webcrawler. If the URLs are crawlable, the Seeders/Sorters nominate them to the End-of-Term (EOT) project, otherwise they add them to the Uncrawlable spreadsheet using the project's Chrome Extension.\n\n\n2. \nResearchers\n\n\nResearchers inspect the \"uncrawlable\" list to confirm that seeders' assessments were correct (that is, that the URL/dataset is indeed uncrawlable), and investigate how the dataset could be best harvested. \nResearch.md\n describes this process in more detail.\n\n\nWe recommend that a Researchers and Harvesters (see below) work together in pairs, as much communication is needed between the two roles. In some case, one same person will fulfill both roles.\n\n\n3. \nHarvesters\n\n\nHarvesters take the \"uncrawlable\" data and try to figure out how to actully capture it based on the recommendations of the Researchers.  This is a complex task which can require substantial technical expertise, and which requires different techniques for different tasks. Harvesters should see the included \nHarvesting Toolkit\n for more details and tools.\n\n\n4. Checkers\n\n\n\n\nNote: This role is currently performed by the Baggers, and does not exist separately.\n\n\nCheckers inspect a harvested dataset and make sure that it is complete. The main question the checkers need to answer is \"will the bag make sense to a scientist\"? Checkers need to have an in-depth understanding of harvesting goals and potential content variations for datasets.\n\n\n5. \nBaggers\n\n\nBaggers perform some quality assurance on the dataset to make sure the content is correct and corresponds to the original URL. Then they package the data into a bagit file (or \"bag\"), which includes basic technical metadata and upload it to final DataRefuge destination.\n\n\n6. \nDescribers\n\n\nNote: This role is still being fine-tuned.\n\n\nDescribers creates a descriptive record in the DataRefuge CKAN repository for each bag. Then they links the record to the bag, and make the record public.\n\n\n\n\nPartners\n\n\nData Rescue is a broad, grassroots effort with support from numerous local and nationwide networks. \nDateRefuge\n and \nEDGI\n partner with local organizers in supporting these events. See more of our institutional partners on the \nData Refuge home page\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#datarescue-workflow-overview", 
            "text": "This document describes the workflow we use for Data Rescue activites as developed by the  DataRefuge project  and  EDGI , both at in-person events and when people work remotely. It explains the process that a url/dataset goes through from the time it has been identified, either by a  seeder   sorter  as \"uncrawlable,\" or by other means, until it is made available as a record in the  datarefuge.org  ckan data catalog. The process involves several stages, and is designed to maximize smooth hand-offs so that each phase is handled by someone with distinct expertise in the area they're tackling, while the data is always being tracked for security.", 
            "title": "DataRescue Workflow -- Overview"
        }, 
        {
            "location": "/#before-you-begin", 
            "text": "We are so glad that you are participating in this project!   If you are an Event Organizer :\n- Learn about what you need to do to prepare the event  here .  If you are a regular participant :\n- Get a role assignment (e.g., Seeder, or Harvester), get account credentials needed for your role, and make sure you have access to the key documents and app needed to do the work. The Event/Remote organizers will tell you how proceed to do all this.\n- Go over the workflow documentation below, in particular the pages corresponding to your role.", 
            "title": "Before you begin"
        }, 
        {
            "location": "/#plan-overview", 
            "text": "", 
            "title": "Plan Overview"
        }, 
        {
            "location": "/#1-seederssorters", 
            "text": "Seeders and Sorters canvass the resources of a given government agency, identifying important URLs. They identify whether those URLs can be crawled by the Internet Archive's webcrawler. If the URLs are crawlable, the Seeders/Sorters nominate them to the End-of-Term (EOT) project, otherwise they add them to the Uncrawlable spreadsheet using the project's Chrome Extension.", 
            "title": "1. Seeders/Sorters"
        }, 
        {
            "location": "/#2-researchers", 
            "text": "Researchers inspect the \"uncrawlable\" list to confirm that seeders' assessments were correct (that is, that the URL/dataset is indeed uncrawlable), and investigate how the dataset could be best harvested.  Research.md  describes this process in more detail.  We recommend that a Researchers and Harvesters (see below) work together in pairs, as much communication is needed between the two roles. In some case, one same person will fulfill both roles.", 
            "title": "2. Researchers"
        }, 
        {
            "location": "/#3-harvesters", 
            "text": "Harvesters take the \"uncrawlable\" data and try to figure out how to actully capture it based on the recommendations of the Researchers.  This is a complex task which can require substantial technical expertise, and which requires different techniques for different tasks. Harvesters should see the included  Harvesting Toolkit  for more details and tools.", 
            "title": "3. Harvesters"
        }, 
        {
            "location": "/#4-checkers", 
            "text": "Note: This role is currently performed by the Baggers, and does not exist separately.  Checkers inspect a harvested dataset and make sure that it is complete. The main question the checkers need to answer is \"will the bag make sense to a scientist\"? Checkers need to have an in-depth understanding of harvesting goals and potential content variations for datasets.", 
            "title": "4. Checkers"
        }, 
        {
            "location": "/#5-baggers", 
            "text": "Baggers perform some quality assurance on the dataset to make sure the content is correct and corresponds to the original URL. Then they package the data into a bagit file (or \"bag\"), which includes basic technical metadata and upload it to final DataRefuge destination.", 
            "title": "5. Baggers"
        }, 
        {
            "location": "/#6-describers", 
            "text": "Note: This role is still being fine-tuned.  Describers creates a descriptive record in the DataRefuge CKAN repository for each bag. Then they links the record to the bag, and make the record public.", 
            "title": "6. Describers"
        }, 
        {
            "location": "/#partners", 
            "text": "Data Rescue is a broad, grassroots effort with support from numerous local and nationwide networks.  DateRefuge  and  EDGI  partner with local organizers in supporting these events. See more of our institutional partners on the  Data Refuge home page .", 
            "title": "Partners"
        }, 
        {
            "location": "/advance-work/", 
            "text": "Organizing an event\n\n\nThis document is meant for DataRescue event organizers. There are lots of ways to prepare for an event. This document highlights only the technical aspects of preparation. There are lots of logistical and other aspects to look out for as well, but this is the minimum needed to use the workflow we propose.  \n\n\nNote that after an event, participants might want to continue the work remotely, and our workflow is designed to make that possible.\n\n\nBefore starting, your team should go through the following steps. \n\n\nThe basics\n\n\n\n\nRead through the entire workflow documentation\n\n\nSign up for the datarefuge slack and make sure there's a channel for your event. \n\n\nDefine your teams. They are usually: Seeders/Sorters, Researchers, Harvesters, Checkers, Baggers, and Describers. Although in some cases, some of the roles can be conflated. \n\n\nIn particular, we recommend that Researchers and Harvesters work very closely with each other, for instance in pairs or in small groups. In some cases, a single person might be both a Researcher and a Harvester.\n\n\nEach team should have team leaders, aka \"guides\". \n\n\nThe event organizers and team leaders should schedule a call with DataRefuge to go over the process. \n\n\nThe event organizers and team leaders for the Seeders and Sorters should also check in with EDGI folks for info about how to make sure that you're seeding and sorting effectively. \n\n\n\n\nNote that the Describers role is being redeveloped at the moment, so it is currently not enabled.\n\n\nPrimer and sub-primer documents\n\n\n\n\nMake sure your event has its designated primer and sub-primer documents\n\n\nThose are documents that will inform the work of the Seeders/Sorters at your event. They will tell them which website or website sections they should be focusing on for URL discovery. \n\n\nAn EDGI coordinator will setup these documents for you.\n\n\n\n\nArchivers app\n\n\n\n\nThe \nArchivers app\n enables us to collectively keep track of all the archiving work being done.\n\n\nIt will also help coordinate the work of different roles (Researchers, Harvesters, Checkers, Baggers, Describers) on each URL.\n\n\nThe app will include URLs coming from two main sources:\n\n\nURLs that were nominated by Seeders at a previous DataRescue event, \n\n\n\n\nURLs that were identified througth the Union of Concerned Scientists survey, which asked the scientific community to list the most vulnerable and important data currently on accessible through federal websites. \n\n\n\n\n\n\nYou need to make sure that:\n\n\n\n\nYour event is listed in the app.\n\n\nTalk to the DataRefuge organizers about this.\n\n\n\n\n\n\nAll the event participants who need it have access to the app (see Credentials section below)\n\n\n\n\nCrawl vs. Harvest: storage location\n\n\n\n\nThe main triage point of the workflow is whether a URL can be automatically crawled, for instance by the Internet Archive, or whether it  needs to be manually harvested. \n\n\nThe crawling process does not require any separate storage management, as the crawlable URLs are nominated to the Internet Archive, who will take care of the actual file storage after they have crawled the pages. See the \nSeeders/Sorters documentation\n for more information on this process. \n\n\nThe datasets harvested through the harvest process and uploaded through are the Archivers app are stored on S3 storage managed by DataRefuge.\n\n\nAt this time there is no direct access to the files stored on S3 for security reason.\n\n\n\n\n\n\n\n\n\n\nCredentials\n\n\n\n\nThe Researchers/Harvesters/Checkers/Baggers need to have an account on the \nArchivers app\n \n\n\nYou will need to generate invites for each one \nwithin the app\n, and paste the URL generated in a slack Direct Message or an email.\n\n\nEach participant invited will automatically \"belongs\" to your event in the app.\n\n\nCheckers and Baggers need to be given explicit privileges in the app to have access to the Checking (i.e. \"Finalize\") and Bagging sections. \n\n\nSeeders/Sorters do not need access to the Archivers app. \n\n\n\n\n\n\n\nOther supplies\n\n\nMake sure you have a few thumb drives to handle very large data sets (above 5 Gigs).\n\n\nAfter the event\n\n\n\n\nParticipants might want to continue the work started at the event remotely\n\n\nThis should be possible, as our workflow is meant to function in-person as well as remotely", 
            "title": "Advance Work"
        }, 
        {
            "location": "/advance-work/#organizing-an-event", 
            "text": "This document is meant for DataRescue event organizers. There are lots of ways to prepare for an event. This document highlights only the technical aspects of preparation. There are lots of logistical and other aspects to look out for as well, but this is the minimum needed to use the workflow we propose.    Note that after an event, participants might want to continue the work remotely, and our workflow is designed to make that possible.  Before starting, your team should go through the following steps.", 
            "title": "Organizing an event"
        }, 
        {
            "location": "/advance-work/#the-basics", 
            "text": "Read through the entire workflow documentation  Sign up for the datarefuge slack and make sure there's a channel for your event.   Define your teams. They are usually: Seeders/Sorters, Researchers, Harvesters, Checkers, Baggers, and Describers. Although in some cases, some of the roles can be conflated.   In particular, we recommend that Researchers and Harvesters work very closely with each other, for instance in pairs or in small groups. In some cases, a single person might be both a Researcher and a Harvester.  Each team should have team leaders, aka \"guides\".   The event organizers and team leaders should schedule a call with DataRefuge to go over the process.   The event organizers and team leaders for the Seeders and Sorters should also check in with EDGI folks for info about how to make sure that you're seeding and sorting effectively.    Note that the Describers role is being redeveloped at the moment, so it is currently not enabled.", 
            "title": "The basics"
        }, 
        {
            "location": "/advance-work/#primer-and-sub-primer-documents", 
            "text": "Make sure your event has its designated primer and sub-primer documents  Those are documents that will inform the work of the Seeders/Sorters at your event. They will tell them which website or website sections they should be focusing on for URL discovery.   An EDGI coordinator will setup these documents for you.", 
            "title": "Primer and sub-primer documents"
        }, 
        {
            "location": "/advance-work/#archivers-app", 
            "text": "The  Archivers app  enables us to collectively keep track of all the archiving work being done.  It will also help coordinate the work of different roles (Researchers, Harvesters, Checkers, Baggers, Describers) on each URL.  The app will include URLs coming from two main sources:  URLs that were nominated by Seeders at a previous DataRescue event,    URLs that were identified througth the Union of Concerned Scientists survey, which asked the scientific community to list the most vulnerable and important data currently on accessible through federal websites.     You need to make sure that:   Your event is listed in the app.  Talk to the DataRefuge organizers about this.    All the event participants who need it have access to the app (see Credentials section below)", 
            "title": "Archivers app"
        }, 
        {
            "location": "/advance-work/#crawl-vs-harvest-storage-location", 
            "text": "The main triage point of the workflow is whether a URL can be automatically crawled, for instance by the Internet Archive, or whether it  needs to be manually harvested.   The crawling process does not require any separate storage management, as the crawlable URLs are nominated to the Internet Archive, who will take care of the actual file storage after they have crawled the pages. See the  Seeders/Sorters documentation  for more information on this process.   The datasets harvested through the harvest process and uploaded through are the Archivers app are stored on S3 storage managed by DataRefuge.  At this time there is no direct access to the files stored on S3 for security reason.", 
            "title": "Crawl vs. Harvest: storage location"
        }, 
        {
            "location": "/advance-work/#credentials", 
            "text": "The Researchers/Harvesters/Checkers/Baggers need to have an account on the  Archivers app    You will need to generate invites for each one  within the app , and paste the URL generated in a slack Direct Message or an email.  Each participant invited will automatically \"belongs\" to your event in the app.  Checkers and Baggers need to be given explicit privileges in the app to have access to the Checking (i.e. \"Finalize\") and Bagging sections.   Seeders/Sorters do not need access to the Archivers app.", 
            "title": "Credentials"
        }, 
        {
            "location": "/advance-work/#other-supplies", 
            "text": "Make sure you have a few thumb drives to handle very large data sets (above 5 Gigs).", 
            "title": "Other supplies"
        }, 
        {
            "location": "/advance-work/#after-the-event", 
            "text": "Participants might want to continue the work started at the event remotely  This should be possible, as our workflow is meant to function in-person as well as remotely", 
            "title": "After the event"
        }, 
        {
            "location": "/seednsort/", 
            "text": "Seeding and Sorting Overview\n\n\nWhat do Seeders/Sorters do?\n\n\nSeeders and Sorters canvass the resources of a given government agency, identifying important URLs. They identify whether those URLs can be crawled by the Internet Archive's webcrawler. If the URLs are crawlable, the Seeders/Sorters nominate them to the End-of-Term (EOT) project, otherwise they add them to the Uncrawlable spreadsheet using the project's Chrome Extension.\n\n\nChoosing the website\n\n\nThe Seeders/Sorters team will use the \nEDGI subprimers\n, or a similar set of resources, to identify important/at risk data. Talk to the DataRescue organizers to learn more.\n\n\nCanvassing the website and evaluating content\n\n\n\n\nStart exploring the website assigned, identifying important URLs.\n\n\nDecide whether the data on a page or website subsection can be automatically captured by the Internet Archive webcrawler.\n\n\nThe best source of information about the seeding and sorting process is represented at \nhttps://envirodatagov.org/\n, see:\n\n\nUnderstanding What the Internet Archive Webcrawler Does\n\n\nSeeding the Internet Archive\u2019s Webcrawler\n\n\n\n\nCrawlable URLs\n\n\n\n\nURLs judged to be possibly crawlable are \"nominated\" (equivalently, \"seeded\") to the End-Of-Term project (EOT), using the \nEDGI Nomination Chrome extension\n or\n  \nbookmarklet\n.\n\n\n\n\nWherever possible, add in the Agency Office Code.\n Talk to the DataRescue organizers to learn more.\n\n\nUncrawlable URLs\n\n\n\n\nIf URL is judged not crawlable, add it to the \"Uncrawlable\" spreadsheet through the Chrome Extension.\n\n\nIn the spreadsheet is automatically associated with a universal unique identifyer (UUID) that was generated in advance.\n\n\nYou can check whether the page or some files are rendered using the Internet Archive's \nWayback Machine Chrome Extension\n\n\n\n\nNot sure?\n\n\n\n\nThis sorting is only provisional: when in doubt seeders nominate the URL \nand\n mark it as possibly not crawlable.", 
            "title": "Seeding"
        }, 
        {
            "location": "/seednsort/#seeding-and-sorting-overview", 
            "text": "", 
            "title": "Seeding and Sorting Overview"
        }, 
        {
            "location": "/seednsort/#what-do-seederssorters-do", 
            "text": "Seeders and Sorters canvass the resources of a given government agency, identifying important URLs. They identify whether those URLs can be crawled by the Internet Archive's webcrawler. If the URLs are crawlable, the Seeders/Sorters nominate them to the End-of-Term (EOT) project, otherwise they add them to the Uncrawlable spreadsheet using the project's Chrome Extension.", 
            "title": "What do Seeders/Sorters do?"
        }, 
        {
            "location": "/seednsort/#choosing-the-website", 
            "text": "The Seeders/Sorters team will use the  EDGI subprimers , or a similar set of resources, to identify important/at risk data. Talk to the DataRescue organizers to learn more.", 
            "title": "Choosing the website"
        }, 
        {
            "location": "/seednsort/#canvassing-the-website-and-evaluating-content", 
            "text": "Start exploring the website assigned, identifying important URLs.  Decide whether the data on a page or website subsection can be automatically captured by the Internet Archive webcrawler.  The best source of information about the seeding and sorting process is represented at  https://envirodatagov.org/ , see:  Understanding What the Internet Archive Webcrawler Does  Seeding the Internet Archive\u2019s Webcrawler", 
            "title": "Canvassing the website and evaluating content"
        }, 
        {
            "location": "/seednsort/#crawlable-urls", 
            "text": "URLs judged to be possibly crawlable are \"nominated\" (equivalently, \"seeded\") to the End-Of-Term project (EOT), using the  EDGI Nomination Chrome extension  or\n   bookmarklet .   Wherever possible, add in the Agency Office Code.  Talk to the DataRescue organizers to learn more.", 
            "title": "Crawlable URLs"
        }, 
        {
            "location": "/seednsort/#uncrawlable-urls", 
            "text": "If URL is judged not crawlable, add it to the \"Uncrawlable\" spreadsheet through the Chrome Extension.  In the spreadsheet is automatically associated with a universal unique identifyer (UUID) that was generated in advance.  You can check whether the page or some files are rendered using the Internet Archive's  Wayback Machine Chrome Extension", 
            "title": "Uncrawlable URLs"
        }, 
        {
            "location": "/seednsort/#not-sure", 
            "text": "This sorting is only provisional: when in doubt seeders nominate the URL  and  mark it as possibly not crawlable.", 
            "title": "Not sure?"
        }, 
        {
            "location": "/research/", 
            "text": "RESEARCH\n\n\nWhat do Researchers do?\n\n\nResearchers inspect the \"uncrawlable\" list to confirm that seeders' assessments were correct (that is, that the URL/dataset is indeed uncrawlable), and investigate how the dataset could be best harvested.\n\n\nGetting set up as a Researcher\n\n\n\n\nSkills recommended for this role: in general, Researchers need to have a good understanding of harvesting goals and have some familiarity with datasets. Ideally they would understand how federal data is organized (e.g. where the \"master\" datasets are vs. the derived partial views of those datasets.\n\n\nThe organizers of the event (in-person or remote) will tell you how to volunteer for the Researcher role, either through slack or a form. \n\n\nAs a result, they will send you an invite to the \nArchivers app\n, which helps us coordinate all the data archiving work we do.\n\n\nClick the invite link, and choose a user name and a password.\n\n\nMake sure you have an account on the DataRefuge slack (or other slack team recommended by your event organizers) This is where people share expertise and answer each other's questions.\n\n\nAsk your event organizer to send you an invite  \n\n\nIf you need any assistance:\n\n\nTalk to your DataRescue Guide if you are at an in-person event\n\n\nOr post questions on Slack in the #Researchers/Harvesters channel (or other channel recommended by your event organizers).\n\n\n\n\nResearchers and Harvesters\n\n\n\n\nResearchers and Harvesters should work very closely together as their work will feed from each other and much communication is needed between the two roles.\n\n\nFor instance they could work in pairs or in small groups. \n\n\nIn some cases, a single person might be both a Researcher and a Harvester.\n\n\nAs a Researcher, make sure to check out the \nHarvesters documentation\n to familiarize yourself with their role.\n\n\n\n\nClaiming a dataset to Research\n\n\n\n\nResearchers work on datasets that were listed as uncrawlable by Seeders.\n\n\nGo to the \nArchivers app\n, click \nURLS\n and then \nRESEARCH\n: all the URLs listed are ready to be researched\n\n\nAvailable URLs are the ones that have not been checked out by someone else, that is, that do not have someone's name in the User column.\n\n\n\n\n\n\nSelect an available URL and click its UUID to get to the detailed view, then click \nCheck out this URL\n. It is now ready for you to work on, and no one else can do anything to it while you have it checked out. \n\n\nWhile you go through the research process, make sure to report as much information as possible in the Archivers app, as this is the place were we collectively keep track of all the work done.\n\n\n\n\nNote: URL vs UUID\n\n\nThe \nURL\n is the link to examine and harvest, and the \nUUID\n is a canonical ID we use to connect the url with the data in question. The UUID will have been generated earlier earlier in the process. UUID stands for Universal Unique Identifier. \n\n\nEvaluating the data\n\n\nGo to the URL, and start inspecting the content.\n\n\nIs the data actually crawlable?\n\n\nAgain, see \nhere\n\nand \nhere\n\nfor a mostly non-technical introduction to the crawler. Some additional\ntechnical notes for answering this:\n- There is no specific file size cutoff on what is crawlable, but large files\n  should be manually captured anyway.\n- Files types like ZIP, PDF, Excel, etc. are crawlable if they are linked.\n- The crawler can only follow HTTP links that appear directly in the DOM at load\n  time. (That is, they should appear as \na href ...\n tags in the page source.)\n  If links are added by Javascript or require submitting a form, they are\n  not crawlable.\n- The crawler does not tolerate web frames (but it straightforward to inspect\n  a page to obtain the content in the frame directly, and then nominate \nthat\n).\n- The crawler recently added the ability to crawl FTP, but we will not rely on\n  this; we will treat resources served over FTP as uncrawlable.\n\n\nWhat to do in each case:\n\n\n\n\nYES\n: If the URL is crawlable or you locate a crawlable URL that accesses the\n  underlying dataset:\n\n\nNominate it: use our\n    \nChrome extension\n,\n    and if that doesn't work then use the\n    \nbookmarklet\n\n\nClick checkbox \nDo not harvest\n in Archivers app.\n \n\n\nNO\n: If it is confirmed not crawlable:\n  \n\n\nSearch agency websites and data.gov for dataset entry points for your dataset collection   \n\n\nTips: Try to understand what data sets are underlying the web pages. Look for related entries in the Archivers app, and ensure that you aren't harvesting a subdirectory if you can harvest the entire directory. Often, data underlying dozens of pages or multiple \"access portal\" apps is also available as one structured data file. \n\n\nMake note of any better entry point in the \nRecommended Approach\nfield, along with any other recommendations on how to proceed with this harvest.\n \n\n\n\n\n\n\nAdd other information that could help the Harvester, such as the format (SQL, FTP, ZIP, PDF Collections, etc.), approximate size, details about what you found, etc. \n\n\nSearch for related URLS that might already have been listed in the Archivers app that might be covered by the same approach, so as not to duplicate work. You can search for them in the \nLink URL\n field.\n\n\nYES AND NO\n: for example, FTP address, mixed content, big data sets:\n \n\n\nNominate it anyway, but also follow the steps for uncrawlable content above.\n\n\nWhile we understand that this may result in some dataset duplication, that is not a concern. We are ensuring that the data is fully preserved and accessible.\n \n\n\n\n\nFinishing up\n\n\n\n\nIn the Archivers app, make sure to fill out as much information as possible to document your work.\n\n\nCheck the Research checkbox (on the right-hand side) to mark that step as completed. \n\n\nClick \nSave\n.\n\n\n\n\nClick \nCheck in URL\n, to release it and allow someone else to work on the next step. \n\n!-- HOW DOES THIS PROCESS WORK NOW:    - If ever a day or more passed  since you originally claimed the item, update the date to today's date. \n\n\n\n\nNote that if more than 2 days have passed since you claimed the dataset and it is still not closed, the \nDate field will turn red\n, signaling that someone else can claim it in your place and start working on it\n\n\nThis will avoid datasets being stuck in the middle of the workflow and not being finalized.--\n\n\n\n\n\n\n\n\nYou're done! Move on to the next URL!", 
            "title": "Researching"
        }, 
        {
            "location": "/research/#research", 
            "text": "", 
            "title": "RESEARCH"
        }, 
        {
            "location": "/research/#what-do-researchers-do", 
            "text": "Researchers inspect the \"uncrawlable\" list to confirm that seeders' assessments were correct (that is, that the URL/dataset is indeed uncrawlable), and investigate how the dataset could be best harvested.", 
            "title": "What do Researchers do?"
        }, 
        {
            "location": "/research/#getting-set-up-as-a-researcher", 
            "text": "Skills recommended for this role: in general, Researchers need to have a good understanding of harvesting goals and have some familiarity with datasets. Ideally they would understand how federal data is organized (e.g. where the \"master\" datasets are vs. the derived partial views of those datasets.  The organizers of the event (in-person or remote) will tell you how to volunteer for the Researcher role, either through slack or a form.   As a result, they will send you an invite to the  Archivers app , which helps us coordinate all the data archiving work we do.  Click the invite link, and choose a user name and a password.  Make sure you have an account on the DataRefuge slack (or other slack team recommended by your event organizers) This is where people share expertise and answer each other's questions.  Ask your event organizer to send you an invite    If you need any assistance:  Talk to your DataRescue Guide if you are at an in-person event  Or post questions on Slack in the #Researchers/Harvesters channel (or other channel recommended by your event organizers).", 
            "title": "Getting set up as a Researcher"
        }, 
        {
            "location": "/research/#researchers-and-harvesters", 
            "text": "Researchers and Harvesters should work very closely together as their work will feed from each other and much communication is needed between the two roles.  For instance they could work in pairs or in small groups.   In some cases, a single person might be both a Researcher and a Harvester.  As a Researcher, make sure to check out the  Harvesters documentation  to familiarize yourself with their role.", 
            "title": "Researchers and Harvesters"
        }, 
        {
            "location": "/research/#claiming-a-dataset-to-research", 
            "text": "Researchers work on datasets that were listed as uncrawlable by Seeders.  Go to the  Archivers app , click  URLS  and then  RESEARCH : all the URLs listed are ready to be researched  Available URLs are the ones that have not been checked out by someone else, that is, that do not have someone's name in the User column.    Select an available URL and click its UUID to get to the detailed view, then click  Check out this URL . It is now ready for you to work on, and no one else can do anything to it while you have it checked out.   While you go through the research process, make sure to report as much information as possible in the Archivers app, as this is the place were we collectively keep track of all the work done.", 
            "title": "Claiming a dataset to Research"
        }, 
        {
            "location": "/research/#note-url-vs-uuid", 
            "text": "The  URL  is the link to examine and harvest, and the  UUID  is a canonical ID we use to connect the url with the data in question. The UUID will have been generated earlier earlier in the process. UUID stands for Universal Unique Identifier.", 
            "title": "Note: URL vs UUID"
        }, 
        {
            "location": "/research/#evaluating-the-data", 
            "text": "Go to the URL, and start inspecting the content.", 
            "title": "Evaluating the data"
        }, 
        {
            "location": "/research/#is-the-data-actually-crawlable", 
            "text": "Again, see  here \nand  here \nfor a mostly non-technical introduction to the crawler. Some additional\ntechnical notes for answering this:\n- There is no specific file size cutoff on what is crawlable, but large files\n  should be manually captured anyway.\n- Files types like ZIP, PDF, Excel, etc. are crawlable if they are linked.\n- The crawler can only follow HTTP links that appear directly in the DOM at load\n  time. (That is, they should appear as  a href ...  tags in the page source.)\n  If links are added by Javascript or require submitting a form, they are\n  not crawlable.\n- The crawler does not tolerate web frames (but it straightforward to inspect\n  a page to obtain the content in the frame directly, and then nominate  that ).\n- The crawler recently added the ability to crawl FTP, but we will not rely on\n  this; we will treat resources served over FTP as uncrawlable.  What to do in each case:   YES : If the URL is crawlable or you locate a crawlable URL that accesses the\n  underlying dataset:  Nominate it: use our\n     Chrome extension ,\n    and if that doesn't work then use the\n     bookmarklet  Click checkbox  Do not harvest  in Archivers app.\n   NO : If it is confirmed not crawlable:\n    Search agency websites and data.gov for dataset entry points for your dataset collection     Tips: Try to understand what data sets are underlying the web pages. Look for related entries in the Archivers app, and ensure that you aren't harvesting a subdirectory if you can harvest the entire directory. Often, data underlying dozens of pages or multiple \"access portal\" apps is also available as one structured data file.   Make note of any better entry point in the  Recommended Approach field, along with any other recommendations on how to proceed with this harvest.\n     Add other information that could help the Harvester, such as the format (SQL, FTP, ZIP, PDF Collections, etc.), approximate size, details about what you found, etc.   Search for related URLS that might already have been listed in the Archivers app that might be covered by the same approach, so as not to duplicate work. You can search for them in the  Link URL  field.  YES AND NO : for example, FTP address, mixed content, big data sets:\n   Nominate it anyway, but also follow the steps for uncrawlable content above.  While we understand that this may result in some dataset duplication, that is not a concern. We are ensuring that the data is fully preserved and accessible.", 
            "title": "Is the data actually crawlable?"
        }, 
        {
            "location": "/research/#finishing-up", 
            "text": "In the Archivers app, make sure to fill out as much information as possible to document your work.  Check the Research checkbox (on the right-hand side) to mark that step as completed.   Click  Save .   Click  Check in URL , to release it and allow someone else to work on the next step.  !-- HOW DOES THIS PROCESS WORK NOW:    - If ever a day or more passed  since you originally claimed the item, update the date to today's date.    Note that if more than 2 days have passed since you claimed the dataset and it is still not closed, the  Date field will turn red , signaling that someone else can claim it in your place and start working on it  This will avoid datasets being stuck in the middle of the workflow and not being finalized.--     You're done! Move on to the next URL!", 
            "title": "Finishing up"
        }, 
        {
            "location": "/harvesting/", 
            "text": "What do Harvesters do?\n\n\nHarvesters take the \"uncrawlable\" data and try to figure out how to actually capture it based on the recommendations of the Researchers. This is a complex task which can require substantial technical expertise, and which requires different techniques for different tasks.\n\n\nImportant notes\n\n\n\n\nResearchers and Harvesters\n\n\nResearchers and Harvesters should work very closely together as their work will feed from each other and much communication is needed between the two roles.\n\n\nFor instance they could work in pairs or in small groups. \n\n\nIn some cases, a single person might be both a Researcher and a Harvester.\n\n\n\n\n\n\n\n\nAs a Harvester, make sure to check out the \nResearchers documentation\n to familiarize yourself with their role.\n\n\n\n\n\n\nThe notion of \"meaningful dataset\"\n\n\n\n\nYour role is to harvest datasets that are complete and \nmeaningful\n. By meaningful we mean: \"will the bag make sense to a scientist\"? \n\n\nFor instance, if a dataset is composed of a spreadsheet without any accompanying key or explanation of what the data represents, it might be completely impossible for a scientist to use it.\n\n\n\n\nGetting set up as a Harvester\n\n\n\n\nSkills recommended for this role: in general, Harvesters need to have some tech skills and a good understanding of harvesting goals.\n\n\nThe organizers of the event (in-person or remote) will tell you how to volunteer for the Harvester role, either through slack or a form. \n\n\nAs a result, they will send you an invite to the \nArchivers app\n, which helps us coordinate all the data archiving work we do.\n\n\nClick the invite link, and choose a user name and a password.\n\n\n\n\n\n\nMake sure you have an account on the DataRefuge slack (or other slack team recommended by your event organizers) This is where people share expertise and answer each other's questions.\n\n\nAsk your event organizer to send you an invite \n\n\nYou might also need to have some other software and utilities set up on your computer, depending on the harvested methods you will use.\n\n\nHarvesters should start by reading this document, which outlines the steps for constructing a proper data archive of the highest possible integrity. The primary focus of this document is on \nsemi-automated harvesting as part of a team\n, and the workflow described is best-suited for volunteers working to preserve small and medium-sized collections. Where possible, we try to link out to other options appropriate to other circumstances.\n\n\nIf you need any assistance:\n\n\nTalk to your DataRescue Guide if you are at an in-person event\n\n\nOr post  questions on Slack in the #Researchers/Harvesters channel (or other channel recommended by your event organizers).\n\n\n\n\n\n\n\n\nHarvesting Toolkit\n\n\nFor in-depth information on tools and techniques to harvest open data, please check EDGI's extensive \ntoolkit\n.\n\n\n1. Claiming a dataset to harvest\n\n\n\n\nYou will work on datasets that were confirmed as unscrawlable by Researchers.\n\n\nGo to the \nArchivers app\n, click \nURLS\n and then \nHARVEST\n: all the URLs listed are ready to be harvested\n\n\nAvailable URLs are the ones that have not been checked out by someone else, that is, that do not have someone's name in the User column.\n\n\nSelect an available URL and click its UUID to get to the detailed view, then click \nCheck out this URL\n. It is now ready for you to work on, and no one else can do anything to it while you have it checked out. \n\n\nWhile you go through the harvesting process, make sure to report as much information as possible in the Archivers app, as this is the place were we collectively keep track of all the work done.\n\n\n\n\nNote: URL vs UUID\n\n\nThe \nURL\n is the link to examine and harvest, and the \nUUID\n is a canonical ID we use to connect the url with the data in question. The UUID will have been generated earlier earlier in the process. UUID stands for Universal Unique Identifier. \n\n\n2a. Classify Source Type \n archivability\n\n\nBefore doing anything, take a minute to understand what you're looking at. It's usually best to have a quick check of the url to confirm that this data in fact not crawlable. Often as part of the harvesting team, you'll be the first person with a higher level of technical knowledge to review the url in question.\n\n\nCheck for false-positives (content that is in fact crawlable)\n\n\nGenerally, any url that returns standard HTML, links to more \nHTML mimetype pages\n, and contains little-to-no non-html content, it's crawlable. \"View source\" from your browser of choice will help see what the crawler itself is seeing. If in fact the data can be crawled, make a note as such in the Google sheet, remove your name from the \"checkout\" column, notify the seeding / crawling team and they will make sure the link is crawled, and move on to another url.\n\n\nSome things to think about while reviewing a url\n\n\n\n\nDoes this page use javascript to render its content, especially to \ngenerate links\n or \ndynamically pull up images and pdf content\n? Crawlers generally cannot parse dynamically-generated content.\n\n\nDoes this url contain links to non-html content? (For example, zip files, pdfs, excel files, etc...)\n\n\nIs this url some sort of interface for a large database or service? (For example, an interactive map, api gateway, etc.)\n\n\nDoes this url contain instructions for connecting to a server, database, or other special source of data?\n\n\n\n\nCheck the terms of service!!!\n\n\nBefore you go any further, it is \nalways\n worth confirming that the data in question is in fact open for archiving. If the terms of service explicitly prohibit archiving, \nmake a note of it\n. Generally archive-a-thons are purposely only aimed at publically available data, but it is easy to follow a link away from a publically-available source onto a site that has different terms of service.\n\n\nData acquired outside terms of service is not usable\n\n\nIf there is harvestable data, the next step is to set up a directory (step three), and then choose the appropriate strategy for archiving!\n\n\n2b. Determine Scale of the Dataset\n\n\nIf the dataset you're looking at is quite large -- say, more than 1000 documents -- capturing it may require more elaborate programming than is described here, and it may be difficult to complete in the timeframe of the event. In that case, you may want to look outside the scope of this document and read the documentation of tools such as the \nEIS WARC archiver\n, which shows how to initiate a larger, fully automated harvest on a web-based virtual machine. Talk to your DataRescue Guide to determine how to best proceed.\n\n\n3. Generate HTML, JSON \n Directory\n\n\nTo get started click \nDownload Zip Starter\n, which will download an empty Zip archive structure for the data you are about to harvest.\nThe structure looks like this:\n\n\nDAFD2E80-965F-4989-8A77-843DE716D899\n    \u251c\u2500\u2500 DAFD2E80-965F-4989-8A77-843DE716D899.html\n    \u251c\u2500\u2500 DAFD2E80-965F-4989-8A77-843DE716D899.json\n    \u251c\u2500\u2500 /tools\n    \u2514\u2500\u2500 /data\n\n\n\nEach row in the above is:\n\n\nA directory named by the UUID\n    \u251c\u2500\u2500 a .html \"web archive\" file of the url for future reference, named with the ID\n    \u251c\u2500\u2500 a .json metadata file that contains relevant metadata, named with the ID\n    \u251c\u2500\u2500 a /tools directory to include any scripts, notes \n files used to acquire the data\n    \u2514\u2500\u2500 a /data directory that contains the data in question\n\n\n\nUUID\n\n\nThe goal is to pass this finalized folder off for \n\"bagging\"\n. We repeatedly use the UUID so that we can programmatically work through this data later. It is important that the ID be copied \nexactly\n wherever it appears, with no leading or trailing spaces, and honoring case-sensitivity.\n\n\n[id].html file\n\n\nThe zip starter archive will automatically include a copy of the page corresponding to the URL. The html file gives the archive a snapshot of the page at the time of archiving which we can use to monitor for changing data in the future, and corrobrate the provenance of the archive itself. We can also use the .html in conjunction with the scripts you'll include in the tools directory to replicate the archive in the future. \n\n\n\n\n\n[id].json file\n\n\nThe json file is one you'll create by hand to create a machine readable record of the archive. This file contains vital data, including the url that was archived, and date of archiving. The \nid.json readme\n goes into much more detail.\n\n\n4. Acquire the Data\n\n\nYour method for doing this will depend on the shape and size of the data you're dealing with. A few methods are described below.\n\n\n4a. Identify Data Links \n acquire them in a wget loop\n\n\nIf you encounter a page that links to lots of data (for example a \"downloads\" page), this approach may well work. It's important to only use this approach when you encounter \ndata\n, for example pdf's, .zip archives, .csv datasets, etc.\n\n\nThe tricky part of this approach is generating a list of urls to download from the page. If you're skilled with using scripts in combination with html-parsers (for example python's wonderful \nbeautiful-soup package\n), go for it. Otherwise, we've included the \njquery-url-extraction guide\n], which has the advantage of working within a browser and can operate on a page that has been modified by javascript.\n\n\nOur example dataset uses jquery-url, \nleveraging that tool to generate a list of urls to feed the wget loop\n.\n\n\n4b. Identify Data Links \n acquire them via WARCFactory\n\n\nFor search results from large document sets, you may need to do more sophisticated \"scraping\" and \"crawling\" -- again, check out tools built at previous events such as the \nEIS WARC archiver\n or the \nEPA Search Utils\n for ideas on how to proceed.\n\n\n4c. FTP download\n\n\nGovernment datasets are often stored on FTP. It's pretty easy to crawl these FTP sites with a simple Python script. Have a look at \ndownload_ftp_tree.py\n as an example. Note that the Internet Archive is doing an FTP crawl, so another option (especially if the dataset is large) would be to nominate this as a seed (though FTP seeds should be nominated \nseparately\n from http seeds).\n\n\n4d. API scrape / Custom Solution\n\n\nIf you encounter an API, chances are you'll have to build some sort of custom solution, or investigate a social angle. For example: asking someone with greater access for a database dump.\n\n\n4e. Automated Full Browser\n\n\nThe last resort of harvesting should be to drive it with a full web browser. It is slower than other approaches such as \nwget\n, \ncurl\n, or a headless browser. Additionally, this implementation is prone to issues where the resulting page is saved before it's done loading. There is a \nruby example\n.\n\n\n5. Write [id].json metadata, add /tools\n\n\nFrom there you'll want to fill out the metadata.json. Use the template below as a guide.\n\n\n\n\nThe json should match the information from the Harvester and use the following format:\n\n\n\n\n{\n   \"Individual source or seed URL\": \"http://www.eia.gov/renewable/data.cfm\",\n   \"UUID\" : \"E30FA3CA-C5CB-41D5-8608-0650D1B6F105\",\n   \"id_agency\" : 2,\n   \"id_subagency\": ,\n   \"id_org\":,\n   \"id_suborg\":,\n   \"Institution facilitating the data capture creation and packaging\": \"Penn Data Refuge\",\n   \"Date of capture\": \"2017-01-17\",\n   \"Federal agency data acquired from\": \"Department of Energy/U.S. Energy Information Administration\",\n   \"Name of resource\": \"Renewable and Alternative Fuels\",\n   \"File formats contained in package\": \".pdf, .zip\",\n   \"Type(s) of content in package\": \"datasets, codebooks\",\n   \"Free text description of capture process\": \"Metadata was generated by viewing page and using spreadsheet descriptions where necessary, data was bulk downloaded from the page using wget -r on the seed URL and then bagged.\",\n   \"Name of package creator\": \"Mallick Hossain and Ben Goldman\"\n   }\n\n - Make sure to save this as a .json file.\n\n\nIn addition, copy any scripts and tools you used into the /tools directory. It may seem strange to copy code multiple times, but this can help later to reconstruct the archiving process for further refinement later on.\n\n\nIt's worth using some judgement here. If a \"script\" you used includes an entire copy of JAVA, or some suite beyond a simple script, it may be better to document your process in a file and leave that in the tools directory instead.\n\n\nTips\n\n\n\n\nIf you encounter a Search bar, try entering \"*\" to check to see if that returns \"all results\".\n\n\nLeave the data unmodified\nDuring the process you may feel inclined to clean things up, add structure to the data, etc. Avoid temptation. Your finished archive will be hashed so we can compare it later for changes, and it's important that we archive original, unmodified content.\n\n\n\n\n6. Uploading the data\n\n\n\n\nZip the all the files pertaining to your dataset within the zip started archive structure and confirm that it is named with the original UUID\n\n\nUpload the zip file by clicking \nUpload\n in the Archivers app, and selecting \nChoose File\n\n\nNote that files beyond 5 Gigs must be uploaded through the more advanced \nGenerate Upload Token\n option. This will require using the aws command line interface.\n\n\nPlease talk to your DataRescue guide/post on Slack in Baggers channel, if you are having issues with this more advanced method.\n\n\n\n\n7. Finishing up\n\n\n\n\nIn the Archivers app, make sure to fill out as much information as possible to document your work.\n\n\nCheck the Harvest checkbox (on the right-hand side) to mark that step as completed. \n\n\nClick \nSave\n.\n\n\n\n\nClick \nCheck in URL\n, to release it and allow someone else to work on the next step. \n\n\n\n\n\n\nYou're done! Move on to the next URL!", 
            "title": "Harvesting"
        }, 
        {
            "location": "/harvesting/#what-do-harvesters-do", 
            "text": "Harvesters take the \"uncrawlable\" data and try to figure out how to actually capture it based on the recommendations of the Researchers. This is a complex task which can require substantial technical expertise, and which requires different techniques for different tasks.", 
            "title": "What do Harvesters do?"
        }, 
        {
            "location": "/harvesting/#important-notes", 
            "text": "Researchers and Harvesters  Researchers and Harvesters should work very closely together as their work will feed from each other and much communication is needed between the two roles.  For instance they could work in pairs or in small groups.   In some cases, a single person might be both a Researcher and a Harvester.     As a Harvester, make sure to check out the  Researchers documentation  to familiarize yourself with their role.    The notion of \"meaningful dataset\"   Your role is to harvest datasets that are complete and  meaningful . By meaningful we mean: \"will the bag make sense to a scientist\"?   For instance, if a dataset is composed of a spreadsheet without any accompanying key or explanation of what the data represents, it might be completely impossible for a scientist to use it.", 
            "title": "Important notes"
        }, 
        {
            "location": "/harvesting/#getting-set-up-as-a-harvester", 
            "text": "Skills recommended for this role: in general, Harvesters need to have some tech skills and a good understanding of harvesting goals.  The organizers of the event (in-person or remote) will tell you how to volunteer for the Harvester role, either through slack or a form.   As a result, they will send you an invite to the  Archivers app , which helps us coordinate all the data archiving work we do.  Click the invite link, and choose a user name and a password.    Make sure you have an account on the DataRefuge slack (or other slack team recommended by your event organizers) This is where people share expertise and answer each other's questions.  Ask your event organizer to send you an invite   You might also need to have some other software and utilities set up on your computer, depending on the harvested methods you will use.  Harvesters should start by reading this document, which outlines the steps for constructing a proper data archive of the highest possible integrity. The primary focus of this document is on  semi-automated harvesting as part of a team , and the workflow described is best-suited for volunteers working to preserve small and medium-sized collections. Where possible, we try to link out to other options appropriate to other circumstances.  If you need any assistance:  Talk to your DataRescue Guide if you are at an in-person event  Or post  questions on Slack in the #Researchers/Harvesters channel (or other channel recommended by your event organizers).", 
            "title": "Getting set up as a Harvester"
        }, 
        {
            "location": "/harvesting/#harvesting-toolkit", 
            "text": "For in-depth information on tools and techniques to harvest open data, please check EDGI's extensive  toolkit .", 
            "title": "Harvesting Toolkit"
        }, 
        {
            "location": "/harvesting/#1-claiming-a-dataset-to-harvest", 
            "text": "You will work on datasets that were confirmed as unscrawlable by Researchers.  Go to the  Archivers app , click  URLS  and then  HARVEST : all the URLs listed are ready to be harvested  Available URLs are the ones that have not been checked out by someone else, that is, that do not have someone's name in the User column.  Select an available URL and click its UUID to get to the detailed view, then click  Check out this URL . It is now ready for you to work on, and no one else can do anything to it while you have it checked out.   While you go through the harvesting process, make sure to report as much information as possible in the Archivers app, as this is the place were we collectively keep track of all the work done.", 
            "title": "1. Claiming a dataset to harvest"
        }, 
        {
            "location": "/harvesting/#note-url-vs-uuid", 
            "text": "The  URL  is the link to examine and harvest, and the  UUID  is a canonical ID we use to connect the url with the data in question. The UUID will have been generated earlier earlier in the process. UUID stands for Universal Unique Identifier.", 
            "title": "Note: URL vs UUID"
        }, 
        {
            "location": "/harvesting/#2a-classify-source-type-archivability", 
            "text": "Before doing anything, take a minute to understand what you're looking at. It's usually best to have a quick check of the url to confirm that this data in fact not crawlable. Often as part of the harvesting team, you'll be the first person with a higher level of technical knowledge to review the url in question.", 
            "title": "2a. Classify Source Type &amp; archivability"
        }, 
        {
            "location": "/harvesting/#check-for-false-positives-content-that-is-in-fact-crawlable", 
            "text": "Generally, any url that returns standard HTML, links to more  HTML mimetype pages , and contains little-to-no non-html content, it's crawlable. \"View source\" from your browser of choice will help see what the crawler itself is seeing. If in fact the data can be crawled, make a note as such in the Google sheet, remove your name from the \"checkout\" column, notify the seeding / crawling team and they will make sure the link is crawled, and move on to another url.", 
            "title": "Check for false-positives (content that is in fact crawlable)"
        }, 
        {
            "location": "/harvesting/#some-things-to-think-about-while-reviewing-a-url", 
            "text": "Does this page use javascript to render its content, especially to  generate links  or  dynamically pull up images and pdf content ? Crawlers generally cannot parse dynamically-generated content.  Does this url contain links to non-html content? (For example, zip files, pdfs, excel files, etc...)  Is this url some sort of interface for a large database or service? (For example, an interactive map, api gateway, etc.)  Does this url contain instructions for connecting to a server, database, or other special source of data?", 
            "title": "Some things to think about while reviewing a url"
        }, 
        {
            "location": "/harvesting/#check-the-terms-of-service", 
            "text": "Before you go any further, it is  always  worth confirming that the data in question is in fact open for archiving. If the terms of service explicitly prohibit archiving,  make a note of it . Generally archive-a-thons are purposely only aimed at publically available data, but it is easy to follow a link away from a publically-available source onto a site that has different terms of service.  Data acquired outside terms of service is not usable  If there is harvestable data, the next step is to set up a directory (step three), and then choose the appropriate strategy for archiving!", 
            "title": "Check the terms of service!!!"
        }, 
        {
            "location": "/harvesting/#2b-determine-scale-of-the-dataset", 
            "text": "If the dataset you're looking at is quite large -- say, more than 1000 documents -- capturing it may require more elaborate programming than is described here, and it may be difficult to complete in the timeframe of the event. In that case, you may want to look outside the scope of this document and read the documentation of tools such as the  EIS WARC archiver , which shows how to initiate a larger, fully automated harvest on a web-based virtual machine. Talk to your DataRescue Guide to determine how to best proceed.", 
            "title": "2b. Determine Scale of the Dataset"
        }, 
        {
            "location": "/harvesting/#3-generate-html-json-directory", 
            "text": "To get started click  Download Zip Starter , which will download an empty Zip archive structure for the data you are about to harvest.\nThe structure looks like this:  DAFD2E80-965F-4989-8A77-843DE716D899\n    \u251c\u2500\u2500 DAFD2E80-965F-4989-8A77-843DE716D899.html\n    \u251c\u2500\u2500 DAFD2E80-965F-4989-8A77-843DE716D899.json\n    \u251c\u2500\u2500 /tools\n    \u2514\u2500\u2500 /data  Each row in the above is:  A directory named by the UUID\n    \u251c\u2500\u2500 a .html \"web archive\" file of the url for future reference, named with the ID\n    \u251c\u2500\u2500 a .json metadata file that contains relevant metadata, named with the ID\n    \u251c\u2500\u2500 a /tools directory to include any scripts, notes   files used to acquire the data\n    \u2514\u2500\u2500 a /data directory that contains the data in question", 
            "title": "3. Generate HTML, JSON &amp; Directory"
        }, 
        {
            "location": "/harvesting/#uuid", 
            "text": "The goal is to pass this finalized folder off for  \"bagging\" . We repeatedly use the UUID so that we can programmatically work through this data later. It is important that the ID be copied  exactly  wherever it appears, with no leading or trailing spaces, and honoring case-sensitivity.", 
            "title": "UUID"
        }, 
        {
            "location": "/harvesting/#idhtml-file", 
            "text": "The zip starter archive will automatically include a copy of the page corresponding to the URL. The html file gives the archive a snapshot of the page at the time of archiving which we can use to monitor for changing data in the future, and corrobrate the provenance of the archive itself. We can also use the .html in conjunction with the scripts you'll include in the tools directory to replicate the archive in the future.", 
            "title": "[id].html file"
        }, 
        {
            "location": "/harvesting/#idjson-file", 
            "text": "The json file is one you'll create by hand to create a machine readable record of the archive. This file contains vital data, including the url that was archived, and date of archiving. The  id.json readme  goes into much more detail.", 
            "title": "[id].json file"
        }, 
        {
            "location": "/harvesting/#4-acquire-the-data", 
            "text": "Your method for doing this will depend on the shape and size of the data you're dealing with. A few methods are described below.", 
            "title": "4. Acquire the Data"
        }, 
        {
            "location": "/harvesting/#4a-identify-data-links-acquire-them-in-a-wget-loop", 
            "text": "If you encounter a page that links to lots of data (for example a \"downloads\" page), this approach may well work. It's important to only use this approach when you encounter  data , for example pdf's, .zip archives, .csv datasets, etc.  The tricky part of this approach is generating a list of urls to download from the page. If you're skilled with using scripts in combination with html-parsers (for example python's wonderful  beautiful-soup package ), go for it. Otherwise, we've included the  jquery-url-extraction guide ], which has the advantage of working within a browser and can operate on a page that has been modified by javascript.  Our example dataset uses jquery-url,  leveraging that tool to generate a list of urls to feed the wget loop .", 
            "title": "4a. Identify Data Links &amp; acquire them in a wget loop"
        }, 
        {
            "location": "/harvesting/#4b-identify-data-links-acquire-them-via-warcfactory", 
            "text": "For search results from large document sets, you may need to do more sophisticated \"scraping\" and \"crawling\" -- again, check out tools built at previous events such as the  EIS WARC archiver  or the  EPA Search Utils  for ideas on how to proceed.", 
            "title": "4b. Identify Data Links &amp; acquire them via WARCFactory"
        }, 
        {
            "location": "/harvesting/#4c-ftp-download", 
            "text": "Government datasets are often stored on FTP. It's pretty easy to crawl these FTP sites with a simple Python script. Have a look at  download_ftp_tree.py  as an example. Note that the Internet Archive is doing an FTP crawl, so another option (especially if the dataset is large) would be to nominate this as a seed (though FTP seeds should be nominated  separately  from http seeds).", 
            "title": "4c. FTP download"
        }, 
        {
            "location": "/harvesting/#4d-api-scrape-custom-solution", 
            "text": "If you encounter an API, chances are you'll have to build some sort of custom solution, or investigate a social angle. For example: asking someone with greater access for a database dump.", 
            "title": "4d. API scrape / Custom Solution"
        }, 
        {
            "location": "/harvesting/#4e-automated-full-browser", 
            "text": "The last resort of harvesting should be to drive it with a full web browser. It is slower than other approaches such as  wget ,  curl , or a headless browser. Additionally, this implementation is prone to issues where the resulting page is saved before it's done loading. There is a  ruby example .", 
            "title": "4e. Automated Full Browser"
        }, 
        {
            "location": "/harvesting/#5-write-idjson-metadata-add-tools", 
            "text": "From there you'll want to fill out the metadata.json. Use the template below as a guide.   The json should match the information from the Harvester and use the following format:   {\n   \"Individual source or seed URL\": \"http://www.eia.gov/renewable/data.cfm\",\n   \"UUID\" : \"E30FA3CA-C5CB-41D5-8608-0650D1B6F105\",\n   \"id_agency\" : 2,\n   \"id_subagency\": ,\n   \"id_org\":,\n   \"id_suborg\":,\n   \"Institution facilitating the data capture creation and packaging\": \"Penn Data Refuge\",\n   \"Date of capture\": \"2017-01-17\",\n   \"Federal agency data acquired from\": \"Department of Energy/U.S. Energy Information Administration\",\n   \"Name of resource\": \"Renewable and Alternative Fuels\",\n   \"File formats contained in package\": \".pdf, .zip\",\n   \"Type(s) of content in package\": \"datasets, codebooks\",\n   \"Free text description of capture process\": \"Metadata was generated by viewing page and using spreadsheet descriptions where necessary, data was bulk downloaded from the page using wget -r on the seed URL and then bagged.\",\n   \"Name of package creator\": \"Mallick Hossain and Ben Goldman\"\n   } \n - Make sure to save this as a .json file.  In addition, copy any scripts and tools you used into the /tools directory. It may seem strange to copy code multiple times, but this can help later to reconstruct the archiving process for further refinement later on.  It's worth using some judgement here. If a \"script\" you used includes an entire copy of JAVA, or some suite beyond a simple script, it may be better to document your process in a file and leave that in the tools directory instead.", 
            "title": "5. Write [id].json metadata, add /tools"
        }, 
        {
            "location": "/harvesting/#tips", 
            "text": "If you encounter a Search bar, try entering \"*\" to check to see if that returns \"all results\".  Leave the data unmodified\nDuring the process you may feel inclined to clean things up, add structure to the data, etc. Avoid temptation. Your finished archive will be hashed so we can compare it later for changes, and it's important that we archive original, unmodified content.", 
            "title": "Tips"
        }, 
        {
            "location": "/harvesting/#6-uploading-the-data", 
            "text": "Zip the all the files pertaining to your dataset within the zip started archive structure and confirm that it is named with the original UUID  Upload the zip file by clicking  Upload  in the Archivers app, and selecting  Choose File  Note that files beyond 5 Gigs must be uploaded through the more advanced  Generate Upload Token  option. This will require using the aws command line interface.  Please talk to your DataRescue guide/post on Slack in Baggers channel, if you are having issues with this more advanced method.", 
            "title": "6. Uploading the data"
        }, 
        {
            "location": "/harvesting/#7-finishing-up", 
            "text": "In the Archivers app, make sure to fill out as much information as possible to document your work.  Check the Harvest checkbox (on the right-hand side) to mark that step as completed.   Click  Save .   Click  Check in URL , to release it and allow someone else to work on the next step.     You're done! Move on to the next URL!", 
            "title": "7. Finishing up"
        }, 
        {
            "location": "/checking/", 
            "text": "Checkers\n\n\nWhat do Checkers do?\n\n\nCheckers inspect a harvested dataset and make sure that it is complete. The main question the checkers need to answer is \"will the bag make sense to a scientist\"? \n\n\nGetting set up as a Checker\n\n\n\n\nSkills recommended for this role: in general, Checkers need to have an in-depth understanding of harvesting goals and potential content variations for datasets.\n\n\nApply to become a Checker \n\n\nBy filling out \nthis form\n \n\n\nNote that an email address is required to apply.\n\n\nNote also that you should be willing to have your real name be associated with the datasets, to follow archival best practices (see \nguidelines on archival best practices for Data Refuge\n for more information).\n\n\n\n\n\n\nThe organizers of the event (in-person or remote) will send you an invite to the \nArchivers app\n, which helps us coordinate all the data archiving work we do.\n\n\nClick the invite link, and choose a user name and a password.    \n\n\n\n\n\n\nMake sure you have an account on the DataRefuge slack where people share expertise and answer each other's questions.\n\n\nAsk your event organizer to send you an invite \n\n\n\n\n\n\nYou might also need to have some other software and utilities set up on your computer, depending methods you will use, when needing to harvest supplemental materials to add to a dataset.\n\n\nIf you need any assistance:\n\n\nTalk to your DataRescue Guide if you are at an in-person event\n\n\nOr post  questions on Slack in the #Checkers channel.\n\n\n\n\nClaiming a dataset for the checking step\n\n\n\n\nYou will work on datasets that were harvested by Harvesters. \n\n\nGo to the \nArchivers app\n, click \nURLS\n and then \nFINALIZE\n: all the URLs listed are ready to be checked\n\n\nAvailable URLs are the ones that have not been checked out by someone else, that is, that do not have someone's name in the User column.\n\n\n\n\n\n\nSelect an available URL and click its UUID to get to the detailed view, then click \nCheck out this URL\n. It is now ready for you to work on, and no one else can do anything to it while you have it checked out. \n\n\nWhile you go through the checking process, make sure to report as much information as possible in the Archivers app, as this is the place were we collectively keep track of all the work done.\n\n\n\n\nNote: URL vs UUID\n\n\nThe \nURL\n is the link to examine and harvest, and the \nUUID\n is a canonical ID we use to connect the url with the data in question. The UUID will have been generated earlier earlier in the process. UUID stands for Universal Unique Identifier. \n\n\nNote: the next few steps below need to be reviewed in light of the new app-driven workflow\n \n\n\nDownloading \n opening the dataset\n\n\n\n\nGo to the URL containing the zipped dataset (provided in cell \"URL from upload of zip\") \n\n\nDownload the zip file to your laptop, and unzip it.\n\n\n\n\nChecking for completeness and meaningfulness\n\n\n\n\nYour role is to inspect the dataset and make sure that it is complete.\n\n\nYou also need to check that the dataset is \nmeaningful\n, that is: \"will the bag make sense to a scientist\"? \n\n\nFor instance, if a dataset is composed of a spreadsheet without any accompanying key or explanation of what the data represents, it might be completely impossible for a scientist to use it.\n\n\n\n\n\n\n\n\nAdding missing items\n\n\n\n\nYou should add any missing file or metadata information to the dataset\n\n\nPlease refer to the \nHarvesting Tookit\n for more details\n\n\n\n\nRe-uploading\n\n\n\n\nIf you have made any changes to the dataset, zip the all the files and upload the new resulting zip file, using the application http://drp-upload.herokuapp.com/\n\n\nMake sure to select the name of your event in the dropdown (and \"remote\" if you are working remotely)\n\n\nNote that files beyond 5 Gigs cannot be uploaded through this method\n\n\nPlease talk to your DataRescue guide/post on Slack in Checkers channel, if you have a larger file\n\n\nThe file you uploaded has now replaced the old version, and it is available at the same url (in cell \"URL from upload of zip\")\n\n\n\n\n\n\nQuality assurance: \n\n\nTo ensure that the zip file was uploaded successfully, go to the URL and download it back to your laptop. \n\n\nUnzip it, open it and spot check to make sure that all the files are there and seem valid.\n\n\n\n\n\n\n\n\nFinishing up\n\n\n\n\nIn the Archivers app, make sure to fill out as much information as possible to document your work.\n\n\nCheck the Finalize checkbox (on the right-hand side) to mark that step as completed. \n\n\nClick \nSave\n.\n\n\n\n\nClick \nCheck in URL\n, to release it and allow someone else to work on the next step. \n\n\n\n\n\n\nYou're done! Move on to the next URL!", 
            "title": "Checking"
        }, 
        {
            "location": "/checking/#checkers", 
            "text": "", 
            "title": "Checkers"
        }, 
        {
            "location": "/checking/#what-do-checkers-do", 
            "text": "Checkers inspect a harvested dataset and make sure that it is complete. The main question the checkers need to answer is \"will the bag make sense to a scientist\"?", 
            "title": "What do Checkers do?"
        }, 
        {
            "location": "/checking/#getting-set-up-as-a-checker", 
            "text": "Skills recommended for this role: in general, Checkers need to have an in-depth understanding of harvesting goals and potential content variations for datasets.  Apply to become a Checker   By filling out  this form    Note that an email address is required to apply.  Note also that you should be willing to have your real name be associated with the datasets, to follow archival best practices (see  guidelines on archival best practices for Data Refuge  for more information).    The organizers of the event (in-person or remote) will send you an invite to the  Archivers app , which helps us coordinate all the data archiving work we do.  Click the invite link, and choose a user name and a password.        Make sure you have an account on the DataRefuge slack where people share expertise and answer each other's questions.  Ask your event organizer to send you an invite     You might also need to have some other software and utilities set up on your computer, depending methods you will use, when needing to harvest supplemental materials to add to a dataset.  If you need any assistance:  Talk to your DataRescue Guide if you are at an in-person event  Or post  questions on Slack in the #Checkers channel.", 
            "title": "Getting set up as a Checker"
        }, 
        {
            "location": "/checking/#claiming-a-dataset-for-the-checking-step", 
            "text": "You will work on datasets that were harvested by Harvesters.   Go to the  Archivers app , click  URLS  and then  FINALIZE : all the URLs listed are ready to be checked  Available URLs are the ones that have not been checked out by someone else, that is, that do not have someone's name in the User column.    Select an available URL and click its UUID to get to the detailed view, then click  Check out this URL . It is now ready for you to work on, and no one else can do anything to it while you have it checked out.   While you go through the checking process, make sure to report as much information as possible in the Archivers app, as this is the place were we collectively keep track of all the work done.", 
            "title": "Claiming a dataset for the checking step"
        }, 
        {
            "location": "/checking/#note-url-vs-uuid", 
            "text": "The  URL  is the link to examine and harvest, and the  UUID  is a canonical ID we use to connect the url with the data in question. The UUID will have been generated earlier earlier in the process. UUID stands for Universal Unique Identifier.   Note: the next few steps below need to be reviewed in light of the new app-driven workflow", 
            "title": "Note: URL vs UUID"
        }, 
        {
            "location": "/checking/#downloading-opening-the-dataset", 
            "text": "Go to the URL containing the zipped dataset (provided in cell \"URL from upload of zip\")   Download the zip file to your laptop, and unzip it.", 
            "title": "Downloading &amp; opening the dataset"
        }, 
        {
            "location": "/checking/#checking-for-completeness-and-meaningfulness", 
            "text": "Your role is to inspect the dataset and make sure that it is complete.  You also need to check that the dataset is  meaningful , that is: \"will the bag make sense to a scientist\"?   For instance, if a dataset is composed of a spreadsheet without any accompanying key or explanation of what the data represents, it might be completely impossible for a scientist to use it.", 
            "title": "Checking for completeness and meaningfulness"
        }, 
        {
            "location": "/checking/#adding-missing-items", 
            "text": "You should add any missing file or metadata information to the dataset  Please refer to the  Harvesting Tookit  for more details", 
            "title": "Adding missing items"
        }, 
        {
            "location": "/checking/#re-uploading", 
            "text": "If you have made any changes to the dataset, zip the all the files and upload the new resulting zip file, using the application http://drp-upload.herokuapp.com/  Make sure to select the name of your event in the dropdown (and \"remote\" if you are working remotely)  Note that files beyond 5 Gigs cannot be uploaded through this method  Please talk to your DataRescue guide/post on Slack in Checkers channel, if you have a larger file  The file you uploaded has now replaced the old version, and it is available at the same url (in cell \"URL from upload of zip\")    Quality assurance:   To ensure that the zip file was uploaded successfully, go to the URL and download it back to your laptop.   Unzip it, open it and spot check to make sure that all the files are there and seem valid.", 
            "title": "Re-uploading"
        }, 
        {
            "location": "/checking/#finishing-up", 
            "text": "In the Archivers app, make sure to fill out as much information as possible to document your work.  Check the Finalize checkbox (on the right-hand side) to mark that step as completed.   Click  Save .   Click  Check in URL , to release it and allow someone else to work on the next step.     You're done! Move on to the next URL!", 
            "title": "Finishing up"
        }, 
        {
            "location": "/bagging/", 
            "text": "Baggers\n\n\nWhat do Baggers do?\n\n\nBaggers do some quality assurance on the dataset to make sure the content is correct and corresponds to what was described in the spreadsheet. Then they package the data into a bagit file (or \"bag\"), which includes basic technical metadata and upload it to final DataRefuge destination.\n\n\nGetting set up as a Bagger\n\n\n\n\nSkills recommended for this role: in general, Baggers need to have some tech skills and a good understanding of harvesting goals.\n\n\nApply to become a Bagger \n\n\nBy filling out \nthis form\n \n\n\nNote that an email address is required to apply.\n\n\nNote also that you should be willing to have your real name be associated with the datasets, to follow archival best practices (see \nguidelines on archival best practices for Data Refuge\n for more information).\n\n\n\n\n\n\nThe organizers of the event (in-person or remote) will send you an invite to the \nArchivers app\n, which helps us coordinate all the data archiving work we do.\n\n\nClick the invite link, and choose a user name and a password.    \n\n\n\n\n\n\nMake sure you have an account on the DataRefuge slack (or other slack team recommended by your event organizers) This is where people share expertise and answer each other's questions.\n\n\nAsk your event organizer to send you an invite \n\n\nGet set up with Python and the Python script to make a bag at the command line https://github.com/LibraryOfCongress/bagit-python\n\n\nIf you need any assistance:\n\n\nTalk to your DataRescue Guide if you are at an in-person event\n\n\nOr post questions on Slack in the #Baggers channel(or other channel recommended by your event organizers).\n\n\n\n\nClaiming a dataset for bagging\n\n\n\n\nYou will work on datasets that were harvested by Harvesters.\n\n\nGo to the \nArchivers app\n, click \nURLS\n and then \nBAG\n: all the URLs listed are ready to be bagged\n\n\nAvailable URLs are the ones that have not been checked out by someone else, that is, that do not have someone's name in the User column.\n\n\n\n\n\n\nSelect an available URL and click its UUID to get to the detailed view, then click \nCheck out this URL\n. It is now ready for you to work on, and no one else can do anything to it while you have it checked out. \n\n\nWhile you go through the bagging process, make sure to report as much information as possible in the Archivers app, as this is the place were we collectively keep track of all the work done.\n\n\n\n\nNote: URL vs UUID\n\n\nThe \nURL\n is the link to examine and harvest, and the \nUUID\n is a canonical ID we use to connect the url with the data in question. The UUID will have been generated earlier earlier in the process. UUID stands for Universal Unique Identifier. \n\n\nDownloading \n opening the dataset\n\n\n\n\nThe zipped dataset that is ready to be bagged is under \nHarvest Url / Location\n in the the Archivers app. Download it to your laptop, and unzip it.\n\n\nExtra check: Is this URL truly ready to bag? \n\n\nWhile everybody is doing their best to provide accurate information, occasionally a URL will be presented as \"ready to bag\", but, in fact, is not. Symptoms include:\n\n\nThere is no value in the \"Harvest Url / Location\" field\n\n\nThere is a note in the Harvest section that seem to indicate that the harvest was only partially performed.\n   -\n In either case, uncheck the \"Harvest\" checkbox, and add a note in the Harvest note, indicating that the URL does not seem ready for bagging and needs to be reviewed by a Harvester.\n\n\n\n\n\n\n\n\nQuality Assurance\n\n\n\n\nConfirm the harvested files: \n\n\nGo to the original URL and check that the dataset is complete and accurate.\n\n\nYou also need to check that the dataset is meaningful, that is: \"will the bag make sense to a scientist\"?\nFor instance, if a dataset is composed of a spreadsheet without any accompanying key or explanation of what the data represents, it might be completely impossible for a scientist to use it.\n\n\nSpot check to make sure the files open properly and are not faulty in any way.\n\n\nConfirm content of Json file\n\n\nThe json should match the information from the Harvester and use the following format:\n\n\n\n\n{\n    \"Individual source or seed URL\": \"http://www.eia.gov/renewable/data.cfm\",\n    \"UUID\" : \"E30FA3CA-C5CB-41D5-8608-0650D1B6F105\",\n    \"id_agency\" : 2,\n    \"id_subagency\": ,\n    \"id_org\":,\n    \"id_suborg\":,\n    \"Institution facilitating the data capture creation and packaging\": \"Penn Data Refuge\",\n    \"Date of capture\": \"2017-01-17\",\n    \"Federal agency data acquired from\": \"Department of Energy/U.S. Energy Information Administration\",\n    \"Name of resource\": \"Renewable and Alternative Fuels\",\n    \"File formats contained in package\": \".pdf, .zip\",\n    \"Type(s) of content in package\": \"datasets, codebooks\",\n    \"Free text description of capture process\": \"Metadata was generated by viewing page and using spreadsheet descriptions where necessary, data was bulk downloaded from the page using wget -r on the seed URL and then bagged.\",\n    \"Name of package creator\": \"Mallick Hossain and Ben Goldman\"\n    }\n\n  - If you make any changes, make sure to save this as a .json file.\n  - Confirm that the json file is within the package with the dataset(s)\n\n\nCreating the bag\n\n\n\n\nRun python command line script which creates the bag\n\n\n\n\nbagit.py --contact-name '[your name]' /directory/to/bag\n\n\n\n\nYou should be left with a 'data' folder (which contains the downloaded content and metadata file) and four separate bagit files\n\n\nbag-info.txt\n\n\nbagit.txt\n\n\nmanifest-md5.txt\n\n\ntagmanifest-md5.txt\n\n\n\n\n\n\nIMPORTANT: It's crucial that you do not move or open the bag once you have created it. This may create hidden files that could make the bag invalid later.\n\n\nRun the following python command lind script to do an initial validation of a bag.\n\n\n\n\nbagit.py --validate [directory/of/bag/to/validate]\n\n\n\n\nIf it comes back as valid, proceed to the next step of creating the zip file and uploading it. If it does not, make a note of the error, review your steps, and re-bag the file. If you continue to get invalid bags, please see a DataRescue guide or reach out to the #bagging Slack channel. \n\n\n\n\nCreating the Zip file and uploading it\n\n\n\n\nZip this entire collection (data folder and bagit files) and confirm that it is named with the row's UUID. \n\n\nWithout moving the file\n, upload the zipped bag using the application http://drp-upload-bagger.herokuapp.com/ using the user ID and password provided\n\n\nMake sure to select the name of your event in the dropdown (and \"remote\" if you are working remotely)\n\n\nThe application will return the location URL for your zip file. \n\n\nThe syntax will be \"[UrlStub]/[UUID].zip\n\n\nCut and paste that URL to the \nBag URL\n field in the Archivers app.\n\n\nNote that files beyond 5 Gigs must be uploaded through the more advanced \nGenerate Upload Token\n option. This will require using the aws command line interface.\n\n\nPlease talk to your DataRescue guide/post on Slack in Baggers channel, if you are having issues with this more advanced method.\n\n\n\n\nQuality assurance and finishing up\n\n\n\n\nOnce the zip file has been fully uploaded, download the bag back to your computer (use the URL provided by the Archiver App) and run the following python command line script for validation\n\n\n\n\nbagit.py --validate [directory/of/bag/to/validate]\n\n\n\n\nIf it comes back as valid, open the bag and spot-check to make sure everything looks the same as when you uploaded it. (This will not affect the validity of the bags already uploaded). If all seems right, proceed to the rest of the quality assurance steps. If it does not come back as valid, make a note of the error, review your steps, and re-bag the file. If you continue to get invalid bags, please see a DataRescue guide or reach out to the #bagging Slack channel. \n\n\nFill out as much information as possible in the \nNotes From Bagging\n field in the Archivers app to document your work.\n\n\nCheck the checkbox that certifies this is a \"well-checked bag\"\n\n\nCheck the Bag checkbox (on the right-hand side) to mark that step as completed. \n\n\nClick \nSave\n.\n\n\nClick \nCheck in URL\n to release it and allow someone else to work on the next step.", 
            "title": "Bagging"
        }, 
        {
            "location": "/bagging/#baggers", 
            "text": "", 
            "title": "Baggers"
        }, 
        {
            "location": "/bagging/#what-do-baggers-do", 
            "text": "Baggers do some quality assurance on the dataset to make sure the content is correct and corresponds to what was described in the spreadsheet. Then they package the data into a bagit file (or \"bag\"), which includes basic technical metadata and upload it to final DataRefuge destination.", 
            "title": "What do Baggers do?"
        }, 
        {
            "location": "/bagging/#getting-set-up-as-a-bagger", 
            "text": "Skills recommended for this role: in general, Baggers need to have some tech skills and a good understanding of harvesting goals.  Apply to become a Bagger   By filling out  this form    Note that an email address is required to apply.  Note also that you should be willing to have your real name be associated with the datasets, to follow archival best practices (see  guidelines on archival best practices for Data Refuge  for more information).    The organizers of the event (in-person or remote) will send you an invite to the  Archivers app , which helps us coordinate all the data archiving work we do.  Click the invite link, and choose a user name and a password.        Make sure you have an account on the DataRefuge slack (or other slack team recommended by your event organizers) This is where people share expertise and answer each other's questions.  Ask your event organizer to send you an invite   Get set up with Python and the Python script to make a bag at the command line https://github.com/LibraryOfCongress/bagit-python  If you need any assistance:  Talk to your DataRescue Guide if you are at an in-person event  Or post questions on Slack in the #Baggers channel(or other channel recommended by your event organizers).", 
            "title": "Getting set up as a Bagger"
        }, 
        {
            "location": "/bagging/#claiming-a-dataset-for-bagging", 
            "text": "You will work on datasets that were harvested by Harvesters.  Go to the  Archivers app , click  URLS  and then  BAG : all the URLs listed are ready to be bagged  Available URLs are the ones that have not been checked out by someone else, that is, that do not have someone's name in the User column.    Select an available URL and click its UUID to get to the detailed view, then click  Check out this URL . It is now ready for you to work on, and no one else can do anything to it while you have it checked out.   While you go through the bagging process, make sure to report as much information as possible in the Archivers app, as this is the place were we collectively keep track of all the work done.", 
            "title": "Claiming a dataset for bagging"
        }, 
        {
            "location": "/bagging/#note-url-vs-uuid", 
            "text": "The  URL  is the link to examine and harvest, and the  UUID  is a canonical ID we use to connect the url with the data in question. The UUID will have been generated earlier earlier in the process. UUID stands for Universal Unique Identifier.", 
            "title": "Note: URL vs UUID"
        }, 
        {
            "location": "/bagging/#downloading-opening-the-dataset", 
            "text": "The zipped dataset that is ready to be bagged is under  Harvest Url / Location  in the the Archivers app. Download it to your laptop, and unzip it.  Extra check: Is this URL truly ready to bag?   While everybody is doing their best to provide accurate information, occasionally a URL will be presented as \"ready to bag\", but, in fact, is not. Symptoms include:  There is no value in the \"Harvest Url / Location\" field  There is a note in the Harvest section that seem to indicate that the harvest was only partially performed.\n   -  In either case, uncheck the \"Harvest\" checkbox, and add a note in the Harvest note, indicating that the URL does not seem ready for bagging and needs to be reviewed by a Harvester.", 
            "title": "Downloading &amp; opening the dataset"
        }, 
        {
            "location": "/bagging/#quality-assurance", 
            "text": "Confirm the harvested files:   Go to the original URL and check that the dataset is complete and accurate.  You also need to check that the dataset is meaningful, that is: \"will the bag make sense to a scientist\"?\nFor instance, if a dataset is composed of a spreadsheet without any accompanying key or explanation of what the data represents, it might be completely impossible for a scientist to use it.  Spot check to make sure the files open properly and are not faulty in any way.  Confirm content of Json file  The json should match the information from the Harvester and use the following format:   {\n    \"Individual source or seed URL\": \"http://www.eia.gov/renewable/data.cfm\",\n    \"UUID\" : \"E30FA3CA-C5CB-41D5-8608-0650D1B6F105\",\n    \"id_agency\" : 2,\n    \"id_subagency\": ,\n    \"id_org\":,\n    \"id_suborg\":,\n    \"Institution facilitating the data capture creation and packaging\": \"Penn Data Refuge\",\n    \"Date of capture\": \"2017-01-17\",\n    \"Federal agency data acquired from\": \"Department of Energy/U.S. Energy Information Administration\",\n    \"Name of resource\": \"Renewable and Alternative Fuels\",\n    \"File formats contained in package\": \".pdf, .zip\",\n    \"Type(s) of content in package\": \"datasets, codebooks\",\n    \"Free text description of capture process\": \"Metadata was generated by viewing page and using spreadsheet descriptions where necessary, data was bulk downloaded from the page using wget -r on the seed URL and then bagged.\",\n    \"Name of package creator\": \"Mallick Hossain and Ben Goldman\"\n    } \n  - If you make any changes, make sure to save this as a .json file.\n  - Confirm that the json file is within the package with the dataset(s)", 
            "title": "Quality Assurance"
        }, 
        {
            "location": "/bagging/#creating-the-bag", 
            "text": "Run python command line script which creates the bag   bagit.py --contact-name '[your name]' /directory/to/bag   You should be left with a 'data' folder (which contains the downloaded content and metadata file) and four separate bagit files  bag-info.txt  bagit.txt  manifest-md5.txt  tagmanifest-md5.txt    IMPORTANT: It's crucial that you do not move or open the bag once you have created it. This may create hidden files that could make the bag invalid later.  Run the following python command lind script to do an initial validation of a bag.   bagit.py --validate [directory/of/bag/to/validate]   If it comes back as valid, proceed to the next step of creating the zip file and uploading it. If it does not, make a note of the error, review your steps, and re-bag the file. If you continue to get invalid bags, please see a DataRescue guide or reach out to the #bagging Slack channel.", 
            "title": "Creating the bag"
        }, 
        {
            "location": "/bagging/#creating-the-zip-file-and-uploading-it", 
            "text": "Zip this entire collection (data folder and bagit files) and confirm that it is named with the row's UUID.   Without moving the file , upload the zipped bag using the application http://drp-upload-bagger.herokuapp.com/ using the user ID and password provided  Make sure to select the name of your event in the dropdown (and \"remote\" if you are working remotely)  The application will return the location URL for your zip file.   The syntax will be \"[UrlStub]/[UUID].zip  Cut and paste that URL to the  Bag URL  field in the Archivers app.  Note that files beyond 5 Gigs must be uploaded through the more advanced  Generate Upload Token  option. This will require using the aws command line interface.  Please talk to your DataRescue guide/post on Slack in Baggers channel, if you are having issues with this more advanced method.", 
            "title": "Creating the Zip file and uploading it"
        }, 
        {
            "location": "/bagging/#quality-assurance-and-finishing-up", 
            "text": "Once the zip file has been fully uploaded, download the bag back to your computer (use the URL provided by the Archiver App) and run the following python command line script for validation   bagit.py --validate [directory/of/bag/to/validate]   If it comes back as valid, open the bag and spot-check to make sure everything looks the same as when you uploaded it. (This will not affect the validity of the bags already uploaded). If all seems right, proceed to the rest of the quality assurance steps. If it does not come back as valid, make a note of the error, review your steps, and re-bag the file. If you continue to get invalid bags, please see a DataRescue guide or reach out to the #bagging Slack channel.   Fill out as much information as possible in the  Notes From Bagging  field in the Archivers app to document your work.  Check the checkbox that certifies this is a \"well-checked bag\"  Check the Bag checkbox (on the right-hand side) to mark that step as completed.   Click  Save .  Click  Check in URL  to release it and allow someone else to work on the next step.", 
            "title": "Quality assurance and finishing up"
        }, 
        {
            "location": "/metadata/", 
            "text": "Describers: CKAN/Metadata\n\n\nWhat do Describers do?\n\n\nDescribers creates a descriptive record in the DataRefuge CKAN repository for each bag. Then they links the record to the bag, and make the record public\n\n\nGetting set up as a Describer\n\n\n\n\nSkills recommended for this role: in general, Describers need to have a good handle on metadata practices \n\n\nApply to become a Describer \n\n\nBy asking your DataRescue guide or by filling out \nthis form\n\n\nNote that an email address is required to apply.\n\n\nNote also that you should be willing to have your real name be associated with the datasets, to follow archival best practices (see \nguidelines on archival best practices for Data Refuge\n for more information).\n\n\n\n\n\n\nThe organizers of the event (in-person or remote) will send you an invite to the \nArchivers app\n, which helps us coordinate all the data archiving work we do.\n\n\nClick the invite link, and choose a user name and a password.    \n\n\n\n\n\n\nMake sure you have an account on the DataRefuge slack (or other slack team recommended by your event organizers) This is where people share expertise and answer each other's questions.\n\n\nAsk your event organizer to send you an invite \n\n\n\n\n\n\nThe organizers will also create an account for you in the CKAN instance at https://www.datarefuge.org/ \n\n\nTest that you can log in successfully\n\n\nGet set up with Python and the Python script to make a bag at the command line https://github.com/LibraryOfCongress/bagit-python\n\n\nIf you need any assistance:\n\n\nTalk to your DataRescue Guide if you are at an in-person event\n\n\nOr post questions on Slack in the #Describers channel (or other channel recommended by your event organizers).     \n\n\n\n\nClaiming a bag\n\n\nNote: these features have not yet been implemented in the Archivers app. It should be there shortly.\n\n  - You will work on datasets that were bagged by Baggers. \n  - Go to the \nArchivers app\n, click \nURLS\n and then \nDESCRIBE\n [currently called \nDone\n, this needs to be changed]: all the URLs listed are ready to be added to the CKAN instance\n    - Available URLs are the ones that have not been checked out by someone else, that is, that do not have someone's name in the User column.\n- Select an available URL and click its UUID to get to the detailed view, then click \nCheck out this URL\n. It is now ready for you to work on, and no one else can do anything to it while you have it checked out. \n\n\nNote: URL vs UUID\n\n\nThe \nURL\n is the link to examine and harvest, and the \nUUID\n is a canonical ID we use to connect the url with the data in question. The UUID will have been generated earlier earlier in the process. UUID stands for Universal Unique Identifier. \n\n\nQA step\n\n\n\n\nIn the Archivers app click on the \nBag URL\n in the Bag section of your dataset.   \n\n\nThe file should start downloading\n\n\nWhen it is downloaded, unzip it \n\n\nSpot check some of the files (make sure they open and look normal, i.e., not garbled)\n\n\nIf the file fails QA:\n\n\nBasically, it needs to go back to Bagging. \n\n\nUncheck the Bagging checkbox\n\n\nAnd make a note in the Bagging note field, explaining in what way the bag failed QA and could a bagger please fix the issue. \n\n\n\n\nCreate new record in CKAN\n\n\n\n\nGo to \nCKAN\n and click Organizations in the top menu\n\n\nChoose the organization (i.e., federal agency) that your dataset belongs to, for instance: \nNOAA\n, and click it.\n\n\nIf the Organization you need does not exist yet, create it by clicking \nAdd Organization\n\n\nClick \"Add Dataset\"\n\n\nStart entering metadata in the new record, following the metadata template below.  \n\n\nTitle: title of dataset, e.g., \"Form EIA-411 Data\"\n\n\nDescription: Usually copied and pasted description found on webpage\n\n\nTags: basic descriptive keywords, e.g., \"electric reliability\", \"electricity\", \"power systems\"\n\n\nLicense:  choose value in dropdown. If there is no indicated license, select 'Other - Public Domain'\n\n\nOrganization: choose value in dropdown, e.g., \"United States Department of Energy\"\n\n\nVisibility: select \"Public\"\n\n\nSource: URL where site is live, also in JSON, e.g., \"http://www.eia.gov/electricity/data/eia411/\"\n\n\nTo decide what value to enter in each field:\n\n\nOpen JSON file that is in the bag you have downloaded; it contains some of the metadata you need\n\n\nGo to the original location of the item on the federal agency website (found in the Json file), to find more more facts about the item such as description, title of the dataset, etc. \n\n\nAlternatively, you can also open the html file that should be included in the bag and is a copy of that original main page. \n\n\n\n\n\n\n\n\n\n\n\n\nEnhancing Existing Metadata\n\n\nThese sites will help you obtain federally-sourced metadata that can be added to the CKAN record for more accurate metadata:\n- EPA\n    - https://www.epa.gov/enviro/facility-registry-service-frs\n    - https://edg.epa.gov/metadata/catalog/main/home.page\n- (Add more organizations as we find their official metadata sources)\n\n\nThese sites are sources of scientific metadata standards to review when choosing keywords:\n\n- GCMD Keywords\n    - wiki.earthdata.nasa.gov/display/cmr/gcmd+keyword+access - downloadable CSV files of the GCMD taxonomies \n- ATRAC\n    - https://www.ncdc.noaa.gov/atrac/index.html - this is a free tool to give access to geographic metadata standards including autopopulating thesauri (GCMD and others commonly used with climate data)\n\n\nLinking the CKAN record to the bag:\n\n\n\n\nClick \"Next: Add Data\" at the bottom of the CKAN form\n\n\nEnter the following information:\n\n\nLink: Bag URL, e.g., \"https://drp-upload-bagger.s3.amazonaws.com/remote/77DD634E-EBCE-412E-88B5-A02B0EF12AF6_2.zip\"\n\n\nName: filename, e.g., \"77DD634E-EBCE-412E-88B5-A02B0EF12AF6_2.zip\"\n\n\nFormat: select \"Zip\"\n\n\n\n\n\n\nClick \"Finish\"\n\n\nTest that the link you just created work by clicking it, and verifying that the file begins to download. \n\n\nNote that you don't need to finish downloading it again.\n\n\n\n\n\n\n\n\nFinishing up\n\n\nNote: these features have not yet been implemented in the Archivers app. It should be there shortly.\n\n- In the Archivers app, add URL to the CKAN record in cell \"CKAN record URL\"\n   - The syntax will be \"https://www.datarefuge.org//dataset/[datasetNameGeneratedByCkan]\"\n- Check the Describe checkbox (on the right-hand side) to mark that step as completed. \n- Click \nSave\n.\n- Click \nCheck in URL\n, to release it and allow someone else to work on the next step. \n\n\nTools possibly of use: JSON Viewers\n\n\n\n\nhttp://www.jsoneditoronline.org/\n\n\nhttp://jsonviewer.stack.hu/", 
            "title": "Describing"
        }, 
        {
            "location": "/metadata/#describers-ckanmetadata", 
            "text": "", 
            "title": "Describers: CKAN/Metadata"
        }, 
        {
            "location": "/metadata/#what-do-describers-do", 
            "text": "Describers creates a descriptive record in the DataRefuge CKAN repository for each bag. Then they links the record to the bag, and make the record public", 
            "title": "What do Describers do?"
        }, 
        {
            "location": "/metadata/#getting-set-up-as-a-describer", 
            "text": "Skills recommended for this role: in general, Describers need to have a good handle on metadata practices   Apply to become a Describer   By asking your DataRescue guide or by filling out  this form  Note that an email address is required to apply.  Note also that you should be willing to have your real name be associated with the datasets, to follow archival best practices (see  guidelines on archival best practices for Data Refuge  for more information).    The organizers of the event (in-person or remote) will send you an invite to the  Archivers app , which helps us coordinate all the data archiving work we do.  Click the invite link, and choose a user name and a password.        Make sure you have an account on the DataRefuge slack (or other slack team recommended by your event organizers) This is where people share expertise and answer each other's questions.  Ask your event organizer to send you an invite     The organizers will also create an account for you in the CKAN instance at https://www.datarefuge.org/   Test that you can log in successfully  Get set up with Python and the Python script to make a bag at the command line https://github.com/LibraryOfCongress/bagit-python  If you need any assistance:  Talk to your DataRescue Guide if you are at an in-person event  Or post questions on Slack in the #Describers channel (or other channel recommended by your event organizers).", 
            "title": "Getting set up as a Describer"
        }, 
        {
            "location": "/metadata/#claiming-a-bag", 
            "text": "Note: these features have not yet been implemented in the Archivers app. It should be there shortly. \n  - You will work on datasets that were bagged by Baggers. \n  - Go to the  Archivers app , click  URLS  and then  DESCRIBE  [currently called  Done , this needs to be changed]: all the URLs listed are ready to be added to the CKAN instance\n    - Available URLs are the ones that have not been checked out by someone else, that is, that do not have someone's name in the User column.\n- Select an available URL and click its UUID to get to the detailed view, then click  Check out this URL . It is now ready for you to work on, and no one else can do anything to it while you have it checked out.", 
            "title": "Claiming a bag"
        }, 
        {
            "location": "/metadata/#note-url-vs-uuid", 
            "text": "The  URL  is the link to examine and harvest, and the  UUID  is a canonical ID we use to connect the url with the data in question. The UUID will have been generated earlier earlier in the process. UUID stands for Universal Unique Identifier.", 
            "title": "Note: URL vs UUID"
        }, 
        {
            "location": "/metadata/#qa-step", 
            "text": "In the Archivers app click on the  Bag URL  in the Bag section of your dataset.     The file should start downloading  When it is downloaded, unzip it   Spot check some of the files (make sure they open and look normal, i.e., not garbled)  If the file fails QA:  Basically, it needs to go back to Bagging.   Uncheck the Bagging checkbox  And make a note in the Bagging note field, explaining in what way the bag failed QA and could a bagger please fix the issue.", 
            "title": "QA step"
        }, 
        {
            "location": "/metadata/#create-new-record-in-ckan", 
            "text": "Go to  CKAN  and click Organizations in the top menu  Choose the organization (i.e., federal agency) that your dataset belongs to, for instance:  NOAA , and click it.  If the Organization you need does not exist yet, create it by clicking  Add Organization  Click \"Add Dataset\"  Start entering metadata in the new record, following the metadata template below.    Title: title of dataset, e.g., \"Form EIA-411 Data\"  Description: Usually copied and pasted description found on webpage  Tags: basic descriptive keywords, e.g., \"electric reliability\", \"electricity\", \"power systems\"  License:  choose value in dropdown. If there is no indicated license, select 'Other - Public Domain'  Organization: choose value in dropdown, e.g., \"United States Department of Energy\"  Visibility: select \"Public\"  Source: URL where site is live, also in JSON, e.g., \"http://www.eia.gov/electricity/data/eia411/\"  To decide what value to enter in each field:  Open JSON file that is in the bag you have downloaded; it contains some of the metadata you need  Go to the original location of the item on the federal agency website (found in the Json file), to find more more facts about the item such as description, title of the dataset, etc.   Alternatively, you can also open the html file that should be included in the bag and is a copy of that original main page.", 
            "title": "Create new record in CKAN"
        }, 
        {
            "location": "/metadata/#enhancing-existing-metadata", 
            "text": "These sites will help you obtain federally-sourced metadata that can be added to the CKAN record for more accurate metadata:\n- EPA\n    - https://www.epa.gov/enviro/facility-registry-service-frs\n    - https://edg.epa.gov/metadata/catalog/main/home.page\n- (Add more organizations as we find their official metadata sources)  These sites are sources of scientific metadata standards to review when choosing keywords: \n- GCMD Keywords\n    - wiki.earthdata.nasa.gov/display/cmr/gcmd+keyword+access - downloadable CSV files of the GCMD taxonomies \n- ATRAC\n    - https://www.ncdc.noaa.gov/atrac/index.html - this is a free tool to give access to geographic metadata standards including autopopulating thesauri (GCMD and others commonly used with climate data)", 
            "title": "Enhancing Existing Metadata"
        }, 
        {
            "location": "/metadata/#linking-the-ckan-record-to-the-bag", 
            "text": "Click \"Next: Add Data\" at the bottom of the CKAN form  Enter the following information:  Link: Bag URL, e.g., \"https://drp-upload-bagger.s3.amazonaws.com/remote/77DD634E-EBCE-412E-88B5-A02B0EF12AF6_2.zip\"  Name: filename, e.g., \"77DD634E-EBCE-412E-88B5-A02B0EF12AF6_2.zip\"  Format: select \"Zip\"    Click \"Finish\"  Test that the link you just created work by clicking it, and verifying that the file begins to download.   Note that you don't need to finish downloading it again.", 
            "title": "Linking the CKAN record to the bag:"
        }, 
        {
            "location": "/metadata/#finishing-up", 
            "text": "Note: these features have not yet been implemented in the Archivers app. It should be there shortly. \n- In the Archivers app, add URL to the CKAN record in cell \"CKAN record URL\"\n   - The syntax will be \"https://www.datarefuge.org//dataset/[datasetNameGeneratedByCkan]\"\n- Check the Describe checkbox (on the right-hand side) to mark that step as completed. \n- Click  Save .\n- Click  Check in URL , to release it and allow someone else to work on the next step.", 
            "title": "Finishing up"
        }, 
        {
            "location": "/metadata/#tools-possibly-of-use-json-viewers", 
            "text": "http://www.jsoneditoronline.org/  http://jsonviewer.stack.hu/", 
            "title": "Tools possibly of use: JSON Viewers"
        }
    ]
}