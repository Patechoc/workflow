{
    "docs": [
        {
            "location": "/", 
            "text": "DataRescue Workflow -- Overview\n\n\nThis document describes the workflow we use for DataRescue activities as developed by the \nDataRefuge project\n and \nEDGI\n, both at in-person events and when people work remotely. It explains the process that a URL/dataset goes through from the time it has been identified, either by a \nSeeder\n as \"uncrawlable,\" or by other means, until it is made available as a record in the \ndatarefuge.org\n ckan data catalog. The process involves several stages, and is designed to maximize smooth hand-offs so that each phase is handled by someone with distinct expertise in the area they're tackling, while the data is always being tracked for security.\n\n\nBefore You Begin\n\n\nWe are so glad that you are participating in this project!\n\n\n\n\n\nIf You Are an Event Organizer\n\n\n\n\nLearn about what you need to do to prepare the event \nhere\n.\n\n\n\n\nIf You Are a Regular Participant\n\n\n\n\nGet a role assignment (e.g. Seeder or Harvester), get account credentials needed for your role, and make sure you have access to the key documents and app needed to do the work. The organizers will instruct you on these steps.\n\n\nCreate an account on the DataRefuge Slack using this \nslack-in\n (or use the Slack team recommended by your event organizers). This is where people share expertise and answer each other's questions.   \n\n\nGo over the workflow documentation below and read the detailed page(s) corresponding to your role.\n\n\n\n\n\n\nPlan Overview\n\n\n1. \nSeeding\n\n\nSeeders canvass the resources of a given government agency, identifying important URLs. They identify whether those URLs can be crawled by the Internet Archive's webcrawler. If the URLs are crawlable, the Seeders nominate them to the End-of-Term (EOT) project. Otherwise, they add them to the Uncrawlable spreadsheet using the project's Chrome Extension.\n\n\n2. \nResearching\n\n\nResearchers inspect the \"uncrawlable\" list to confirm that Seeders' assessments were correct (that is, that the URL/dataset is indeed uncrawlable), and investigate how the dataset could be best harvested. \nResearch.md\n describes this process in more detail.\n\n\nWe recommend that Researchers and Harvesters (see below) work together in pairs, as much communication is needed between the two roles. In some cases, one person will fulfill both roles.\n\n\n3. \nHarvesting\n\n\nHarvesters take the \"uncrawlable\" data and try to figure out how to actually capture it based on the recommendations of the Researchers. This is a complex task which can require substantial technical expertise, and which requires different techniques for different tasks. Harvesters should see the included \nHarvesting Toolkit\n for more details and tools.\n\n\n4. Checking\n\n\nNote: This role is currently performed by the Baggers, and does not exist separately.\n\n\nCheckers inspect a harvested dataset and make sure that it is complete. The main question the checkers need to answer is \"will the bag make sense to a scientist\"? Checkers need to have an in-depth understanding of harvesting goals and potential content variations for datasets.\n\n\n5. \nBagging\n\n\nBaggers perform some quality assurance on the dataset to make sure the content is correct and corresponds to the original URL. Then they package the data into a bagit file (or \"bag\"), which includes basic technical metadata, and upload it to the final DataRefuge destination.\n\n\n6. \nDescribing\n\n\nDescribers create a descriptive record in the DataRefuge CKAN repository for each bag. Then they link the record to the bag and make the record public.\n\n\n\n\nPartners\n\n\n HEAD\nDataRescue is a broad, grassroots effort with support from numerous local and nationwide networks. \nDataRefuge\n and \nEDGI\n partner with local organizers in supporting these events. See more of our institutional partners on the \nDataRefuge home page\n.\n=======\nData Rescue is a broad, grassroots effort with support from numerous local and nationwide networks. \nDataRefuge\n and \nEDGI\n partner with local organizers in supporting these events. See more of our institutional partners on the \nData Refuge home page\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmaster", 
            "title": "Home"
        }, 
        {
            "location": "/#datarescue-workflow-overview", 
            "text": "This document describes the workflow we use for DataRescue activities as developed by the  DataRefuge project  and  EDGI , both at in-person events and when people work remotely. It explains the process that a URL/dataset goes through from the time it has been identified, either by a  Seeder  as \"uncrawlable,\" or by other means, until it is made available as a record in the  datarefuge.org  ckan data catalog. The process involves several stages, and is designed to maximize smooth hand-offs so that each phase is handled by someone with distinct expertise in the area they're tackling, while the data is always being tracked for security.", 
            "title": "DataRescue Workflow -- Overview"
        }, 
        {
            "location": "/#before-you-begin", 
            "text": "We are so glad that you are participating in this project!", 
            "title": "Before You Begin"
        }, 
        {
            "location": "/#if-you-are-an-event-organizer", 
            "text": "Learn about what you need to do to prepare the event  here .", 
            "title": "If You Are an Event Organizer"
        }, 
        {
            "location": "/#if-you-are-a-regular-participant", 
            "text": "Get a role assignment (e.g. Seeder or Harvester), get account credentials needed for your role, and make sure you have access to the key documents and app needed to do the work. The organizers will instruct you on these steps.  Create an account on the DataRefuge Slack using this  slack-in  (or use the Slack team recommended by your event organizers). This is where people share expertise and answer each other's questions.     Go over the workflow documentation below and read the detailed page(s) corresponding to your role.", 
            "title": "If You Are a Regular Participant"
        }, 
        {
            "location": "/#plan-overview", 
            "text": "", 
            "title": "Plan Overview"
        }, 
        {
            "location": "/#1-seeding", 
            "text": "Seeders canvass the resources of a given government agency, identifying important URLs. They identify whether those URLs can be crawled by the Internet Archive's webcrawler. If the URLs are crawlable, the Seeders nominate them to the End-of-Term (EOT) project. Otherwise, they add them to the Uncrawlable spreadsheet using the project's Chrome Extension.", 
            "title": "1. Seeding"
        }, 
        {
            "location": "/#2-researching", 
            "text": "Researchers inspect the \"uncrawlable\" list to confirm that Seeders' assessments were correct (that is, that the URL/dataset is indeed uncrawlable), and investigate how the dataset could be best harvested.  Research.md  describes this process in more detail.  We recommend that Researchers and Harvesters (see below) work together in pairs, as much communication is needed between the two roles. In some cases, one person will fulfill both roles.", 
            "title": "2. Researching"
        }, 
        {
            "location": "/#3-harvesting", 
            "text": "Harvesters take the \"uncrawlable\" data and try to figure out how to actually capture it based on the recommendations of the Researchers. This is a complex task which can require substantial technical expertise, and which requires different techniques for different tasks. Harvesters should see the included  Harvesting Toolkit  for more details and tools.", 
            "title": "3. Harvesting"
        }, 
        {
            "location": "/#4-checking", 
            "text": "Note: This role is currently performed by the Baggers, and does not exist separately.  Checkers inspect a harvested dataset and make sure that it is complete. The main question the checkers need to answer is \"will the bag make sense to a scientist\"? Checkers need to have an in-depth understanding of harvesting goals and potential content variations for datasets.", 
            "title": "4. Checking"
        }, 
        {
            "location": "/#5-bagging", 
            "text": "Baggers perform some quality assurance on the dataset to make sure the content is correct and corresponds to the original URL. Then they package the data into a bagit file (or \"bag\"), which includes basic technical metadata, and upload it to the final DataRefuge destination.", 
            "title": "5. Bagging"
        }, 
        {
            "location": "/#6-describing", 
            "text": "Describers create a descriptive record in the DataRefuge CKAN repository for each bag. Then they link the record to the bag and make the record public.", 
            "title": "6. Describing"
        }, 
        {
            "location": "/#partners", 
            "text": "HEAD\nDataRescue is a broad, grassroots effort with support from numerous local and nationwide networks.  DataRefuge  and  EDGI  partner with local organizers in supporting these events. See more of our institutional partners on the  DataRefuge home page .\n=======\nData Rescue is a broad, grassroots effort with support from numerous local and nationwide networks.  DataRefuge  and  EDGI  partner with local organizers in supporting these events. See more of our institutional partners on the  Data Refuge home page .         master", 
            "title": "Partners"
        }, 
        {
            "location": "/advance-work/", 
            "text": "Organizing an Event\n\n\nThis document is meant for DataRescue event organizers. There are lots of ways to prepare for an event. This document highlights only the technical aspects of preparation. There are many logistical and other aspects to prepare for as well, but this is the minimum needed to use the workflow we propose.\n\n\nNote that after an event, participants might want to continue the work remotely. Our workflow is designed to make that possible.\n\n\nBefore starting, your team should go through the following steps.\n\n\nThe Basics\n\n\n\n\nRead through the entire workflow documentation.\n\n\nSign up for the DataRefuge Slack and make sure there's a channel for your event.\n\n\nDefine your teams. They are usually: Seeders, Researchers, Harvesters, Checkers, Baggers, and Describers. Although in some cases, some of the roles can be conflated\n\n\nIn particular, we recommend that Researchers and Harvesters work very closely with each other, e.g. in pairs or small groups. In some cases, a single person might be both a Researcher and a Harvester.\n\n\nEach team should have team leaders, aka \"guides\".\n\n\n\n\n\n\nThe event organizers and team leaders should schedule a call with DataRefuge to go over the process.\n\n\nThe event organizers and team leaders for the Seeders should also check in with EDGI folks for info about how to make sure that you're seeding and sorting effectively.\n\n\n\n\nNote that the Describers role is being redeveloped at the moment, so it is currently not enabled.\n\n\nPrimer and Sub-Primer Documents\n\n\n\n\nMake sure your event has its designated primer and sub-primer documents.\n\n\nThose documents will inform the work of the Seeders at your event. They will tell them which website or website sections they should be focusing on for URL discovery.\n\n\nAn EDGI coordinator will set up these documents for you.\n\n\n\n\nArchivers App\n\n\n\n\nThe \nArchivers app\n enables us to collectively keep track of all the archiving work being done.\n\n\nIt also helps coordinate the work of different roles (Researchers, Harvesters, Checkers, Baggers, Describers) on each URL.\n\n\n\n\n\n\nThe app includes URLs coming from two main sources:\n\n\nURLs that were nominated by Seeders at a previous DataRescue event.\n\n\nURLs that were identified through the Union of Concerned Scientists survey, which asked the scientific community to list the most vulnerable and important data currently accessible through federal websites.\n\n\n\n\n\n\nYou need to make sure that:\n\n\nYour event is listed in the app (talk to the DataRefuge organizers about this).\n\n\nAll the event participants who need it have access to the app (see Credentials section below).\n\n\n\n\n\n\n\n\nCrawl vs. Harvest: Storage Location\n\n\n\n\nThe main triage point of the workflow is whether a URL can be automatically crawled by the Internet Archive or whether it needs to be manually harvested.\n\n\nThe crawling process does not require any separate storage management, as the crawlable URLs are nominated to the Internet Archive, which will take care of the actual file storage after the URL is crawled. See the \nSeeders documentation\n for more information on this process.\n\n\nThe datasets harvested and uploaded through the Archivers app are stored on S3 storage managed by DataRefuge.\n\n\nAt this time there is no direct access to the files stored on S3, for security reason.\n\n\n\n\n\n\n\n\n\n\nCredentials\n\n\n\n\nThe Researchers/Harvesters/Checkers/Baggers need to have an account on the \nArchivers app\n.\n\n\nYou will need to generate invites for each one \nwithin the app\n, and paste the URL generated in a Slack Direct Message or email.\n\n\nEach participant invited will automatically \"belong\" to your event in the app.\n\n\n\n\n\n\nCheckers and Baggers need to be given explicit privileges in the app to have access to the Checking (i.e. \"Finalize\") and Bagging sections.\n\n\nSeeders do not need access to the Archivers app.\n\n\n\n\n\n\n\nOther Supplies\n\n\nMake sure you have a few thumb drives to handle very large data sets (above 5 Gigs).\n\n\nAfter the Event\n\n\n\n\nParticipants might want to continue the work started at the event remotely.\n\n\nThis should be possible, as our workflow is meant to function in-person as well as remotely.", 
            "title": "Advance Work"
        }, 
        {
            "location": "/advance-work/#organizing-an-event", 
            "text": "This document is meant for DataRescue event organizers. There are lots of ways to prepare for an event. This document highlights only the technical aspects of preparation. There are many logistical and other aspects to prepare for as well, but this is the minimum needed to use the workflow we propose.  Note that after an event, participants might want to continue the work remotely. Our workflow is designed to make that possible.  Before starting, your team should go through the following steps.", 
            "title": "Organizing an Event"
        }, 
        {
            "location": "/advance-work/#the-basics", 
            "text": "Read through the entire workflow documentation.  Sign up for the DataRefuge Slack and make sure there's a channel for your event.  Define your teams. They are usually: Seeders, Researchers, Harvesters, Checkers, Baggers, and Describers. Although in some cases, some of the roles can be conflated  In particular, we recommend that Researchers and Harvesters work very closely with each other, e.g. in pairs or small groups. In some cases, a single person might be both a Researcher and a Harvester.  Each team should have team leaders, aka \"guides\".    The event organizers and team leaders should schedule a call with DataRefuge to go over the process.  The event organizers and team leaders for the Seeders should also check in with EDGI folks for info about how to make sure that you're seeding and sorting effectively.   Note that the Describers role is being redeveloped at the moment, so it is currently not enabled.", 
            "title": "The Basics"
        }, 
        {
            "location": "/advance-work/#primer-and-sub-primer-documents", 
            "text": "Make sure your event has its designated primer and sub-primer documents.  Those documents will inform the work of the Seeders at your event. They will tell them which website or website sections they should be focusing on for URL discovery.  An EDGI coordinator will set up these documents for you.", 
            "title": "Primer and Sub-Primer Documents"
        }, 
        {
            "location": "/advance-work/#archivers-app", 
            "text": "The  Archivers app  enables us to collectively keep track of all the archiving work being done.  It also helps coordinate the work of different roles (Researchers, Harvesters, Checkers, Baggers, Describers) on each URL.    The app includes URLs coming from two main sources:  URLs that were nominated by Seeders at a previous DataRescue event.  URLs that were identified through the Union of Concerned Scientists survey, which asked the scientific community to list the most vulnerable and important data currently accessible through federal websites.    You need to make sure that:  Your event is listed in the app (talk to the DataRefuge organizers about this).  All the event participants who need it have access to the app (see Credentials section below).", 
            "title": "Archivers App"
        }, 
        {
            "location": "/advance-work/#crawl-vs-harvest-storage-location", 
            "text": "The main triage point of the workflow is whether a URL can be automatically crawled by the Internet Archive or whether it needs to be manually harvested.  The crawling process does not require any separate storage management, as the crawlable URLs are nominated to the Internet Archive, which will take care of the actual file storage after the URL is crawled. See the  Seeders documentation  for more information on this process.  The datasets harvested and uploaded through the Archivers app are stored on S3 storage managed by DataRefuge.  At this time there is no direct access to the files stored on S3, for security reason.", 
            "title": "Crawl vs. Harvest: Storage Location"
        }, 
        {
            "location": "/advance-work/#credentials", 
            "text": "The Researchers/Harvesters/Checkers/Baggers need to have an account on the  Archivers app .  You will need to generate invites for each one  within the app , and paste the URL generated in a Slack Direct Message or email.  Each participant invited will automatically \"belong\" to your event in the app.    Checkers and Baggers need to be given explicit privileges in the app to have access to the Checking (i.e. \"Finalize\") and Bagging sections.  Seeders do not need access to the Archivers app.", 
            "title": "Credentials"
        }, 
        {
            "location": "/advance-work/#other-supplies", 
            "text": "Make sure you have a few thumb drives to handle very large data sets (above 5 Gigs).", 
            "title": "Other Supplies"
        }, 
        {
            "location": "/advance-work/#after-the-event", 
            "text": "Participants might want to continue the work started at the event remotely.  This should be possible, as our workflow is meant to function in-person as well as remotely.", 
            "title": "After the Event"
        }, 
        {
            "location": "/seeding/", 
            "text": "What Do Seeders Do?\n\n\nSeeders canvass the resources of a given government agency, identifying important URLs. They identify whether those URLs can be crawled by the Internet Archive's web crawler. If the URLs are crawlable, the Seeders nominate them to the \nEnd of Term (EOT) Web Archive\n. Otherwise, they add them to the Uncrawlable spreadsheet using the project's Chrome Extension.\n\n\n\n  \nRecommended Skills\n \n  \n  Consider this path if you\u2019re comfortable browsing the web and have great attention to detail. An understanding of how web pages are structured will help you with this task.\n\n\n\n\nChoosing the Website\n\n\nSeeders use the \nEDGI Archiving Primers\n, or a similar set of resources, to identify important and at-risk data. Talk to the DataRescue organizers to learn more.\n\n\nCanvassing the Website and Evaluating Content\n\n\n\n\nStart exploring the website assigned, identifying important URLs.\n\n\nDecide whether the data on a page or website subsection can be automatically captured by the Internet Archive web crawler.\n\n\nEDGI's Guides\n have information critical to the seeding and sorting process:\n\n\nUnderstanding the Internet Archive Web Crawler\n\n\nSeeding the Internet Archive\u2019s Web Crawler\n\n\n\n\n\n\n\n\nCrawlable URLs\n\n\n\n\nURLs judged to be crawlable are \"nominated\" (equivalently, \"seeded\") to the End of Term project (EOT), using the \nEDGI Nomination Chrome extension\n or\n  \nbookmarklet\n.\n\n\n\n\nWherever possible, add in the Agency Office Code from the sub-primer.\n Talk to the DataRescue organizers to learn more.\n\n\nUncrawlable URLs\n\n\n\n\nIf URL is judged not crawlable, add it to the \"Uncrawlable\" spreadsheet through the Chrome Extension.\n\n\nIn the spreadsheet the URL is automatically associated with a universal unique identifyer (UUID) that was generated in advance.\n\n\nYou can check whether the page or some files are archived using the Internet Archive's \nWayback Machine Chrome Extension\n\n\n\n\nNot Sure?\n\n\n\n\nThis sorting is only provisional; when in doubt, Seeders nominate the URL \nand\n mark it as possibly not crawlable.", 
            "title": "Seeding"
        }, 
        {
            "location": "/seeding/#what-do-seeders-do", 
            "text": "Seeders canvass the resources of a given government agency, identifying important URLs. They identify whether those URLs can be crawled by the Internet Archive's web crawler. If the URLs are crawlable, the Seeders nominate them to the  End of Term (EOT) Web Archive . Otherwise, they add them to the Uncrawlable spreadsheet using the project's Chrome Extension.  \n   Recommended Skills     \n  Consider this path if you\u2019re comfortable browsing the web and have great attention to detail. An understanding of how web pages are structured will help you with this task.", 
            "title": "What Do Seeders Do?"
        }, 
        {
            "location": "/seeding/#choosing-the-website", 
            "text": "Seeders use the  EDGI Archiving Primers , or a similar set of resources, to identify important and at-risk data. Talk to the DataRescue organizers to learn more.", 
            "title": "Choosing the Website"
        }, 
        {
            "location": "/seeding/#canvassing-the-website-and-evaluating-content", 
            "text": "Start exploring the website assigned, identifying important URLs.  Decide whether the data on a page or website subsection can be automatically captured by the Internet Archive web crawler.  EDGI's Guides  have information critical to the seeding and sorting process:  Understanding the Internet Archive Web Crawler  Seeding the Internet Archive\u2019s Web Crawler", 
            "title": "Canvassing the Website and Evaluating Content"
        }, 
        {
            "location": "/seeding/#crawlable-urls", 
            "text": "URLs judged to be crawlable are \"nominated\" (equivalently, \"seeded\") to the End of Term project (EOT), using the  EDGI Nomination Chrome extension  or\n   bookmarklet .   Wherever possible, add in the Agency Office Code from the sub-primer.  Talk to the DataRescue organizers to learn more.", 
            "title": "Crawlable URLs"
        }, 
        {
            "location": "/seeding/#uncrawlable-urls", 
            "text": "If URL is judged not crawlable, add it to the \"Uncrawlable\" spreadsheet through the Chrome Extension.  In the spreadsheet the URL is automatically associated with a universal unique identifyer (UUID) that was generated in advance.  You can check whether the page or some files are archived using the Internet Archive's  Wayback Machine Chrome Extension", 
            "title": "Uncrawlable URLs"
        }, 
        {
            "location": "/seeding/#not-sure", 
            "text": "This sorting is only provisional; when in doubt, Seeders nominate the URL  and  mark it as possibly not crawlable.", 
            "title": "Not Sure?"
        }, 
        {
            "location": "/researching/", 
            "text": "What Do Researchers Do?\n\n\nResearchers review \"uncrawlables\" identified during \nSeeding\n, confirm the URL/dataset is indeed uncrawlable, and investigate how the dataset could be best harvested. Researchers need to have a good understanding of harvesting goals and have some familiarity with datasets.\n\n\n\n  \nRecommended Skills\n \n  \n  Consider this path if you have strong front-end web experience and enjoy research. An understanding of how federal data is organized (e.g. where \"master\" datasets are) would be valuable.\n\n\n\n\nGetting Set up as a Researcher\n\n\n\n\nEvent organizers (in-person or remote) will tell you how to volunteer for the Researcher role, either through Slack or a form.\n\n\nAs a result, they will send you an invite to the \nArchivers app\n, which helps us coordinate all the data archiving work we do.\n\n\nClick the invite link, and choose a user name and a password.\n\n\n\n\n\n\nCreate an account on the DataRefuge Slack using this \nslack-in\n or use the Slack team recommended by your event organizers. This is where people share expertise and answer each other's questions. \n\n\nIf you need any assistance:\n\n\nTalk to your DataRescue guide if you are at an in-person event\n\n\nOr post questions on Slack in the \n#general\n channel (or other channel recommended by your event organizers).\n\n\n\n\n\n\n\n\nResearchers and Harvesters\n\n\n\n\nResearchers and Harvesters should work very closely together as their work will feed from each other and much communication is needed between the two roles.\n\n\nIt may be most effective for Researchers and Harvesters to work together in pairs or small groups. In some cases, a single person might be both a Researcher and a Harvester.\n\n\nAs a Researcher, make sure to check out the \nHarvesters documentation\n to familiarize yourself with their role.\n\n\n\n\nClaiming a Dataset to Research\n\n\n\n\nResearchers work on datasets that were listed as uncrawlable by Seeders.\n\n\nGo to the \nArchivers app\n, click \nURLS\n and then \nRESEARCH\n: all the URLs listed are ready to be researched.\n\n\nAvailable URLs are ones that have not been checked out by someone else, i.e. that do not have someone's name in the User column.\n\n\n\n\n\n\nSelect an available URL and click its UUID to get to the detailed view, then click \nCheck out this URL\n. It is now ready for you to work on, and no one else can do anything to it while you have it checked out.\n\n\nWhile you go through the research process, make sure to report as much information as possible in the Archivers app, as this is the place were we collectively keep track of all the work done.\n\n\n\n\n\n  \nNote: URL vs UUID\n \n  \n  The \nURL\n is the link to examine and harvest, and the \nUUID\n is a canonical ID we use to connect the URL with the data in question. The UUID will have been generated earlier in the process. UUID stands for Universal Unique Identifier.\n\n\n\n\nEvaluating the Data\n\n\nGo to the URL, and start inspecting the content.\n\n\nIs the data actually crawlable?\n\n\nAgain, see \nEDGI's Guides\n for a mostly non-technical introduction to the crawler:\n\n\n\n\nUnderstanding the Internet Archive Web Crawler\n\n\nSeeding the Internet Archive\u2019s Web Crawler\n\n\n\n\nSome additional technical notes for answering this:\n\n\n\n\nThere is no specific file size cutoff for what is crawlable, but large files should be manually captured anyway.\n\n\nFile types like ZIP, PDF, Excel, etc. are crawlable if they are linked.\n\n\nThe crawler can only follow HTTP links that appear directly in the DOM at load time. (That is, they should appear as \na href ...\n tags in the page source.)\nIf links are added by JavaScript or require submitting a form, they are not crawlable.\n\n\nThe crawler does not tolerate web frames (but it is straightforward to inspect a page to obtain the content in the frame directly, and then nominate \nthat\n).\n\n\nThe crawler recently added the ability to crawl FTP, but we will not rely on this; we will treat resources served over FTP as uncrawlable.\n\n\n\n\nYES:\n\n\nIf the URL is crawlable or you locate a crawlable URL that accesses the underlying dataset:\n\n\n\n\nNominate it using our\n  \nChrome extension\n or the \nbookmarklet\n.\n\n\nClick the \nDo not harvest\n checkbox in the Archivers app.\n\n\n\n\n\nNO:\n\n\nIf it is confirmed not crawlable:\n\n\n\n\n\nSearch agency websites and data.gov for dataset entry points for your dataset collection.\n\n\nTips: Try to understand what datasets are underlying the web pages. Look for related entries in the Archivers app, and ensure that you aren't harvesting a subdirectory if you can harvest the entire directory. Often, data underlying dozens of pages or multiple \"access portal\" apps is also available as one structured data file.\n\n\nMake note of any better entry point in the \nRecommended Approach\nfield, along with any other recommendations on how to proceed with this harvest.\n\n\n\n\n\n\n\nAdd other information that could help the Harvester, such as the format (SQL, FTP, ZIP, PDF collections, etc.), approximate size, details about what you found, etc.\n\n\nSearch for related URLs that might already have been listed in the Archivers app that might be covered by the same approach, so as not to duplicate work. You can search for them in the \nLink URL\n field.\n\n\n\n\nYES and NO:\n\n\nFor example, FTP address, mixed content, big data sets:\n\n\n\n\n\nNominate it anyway, but also follow the steps for uncrawlable content above.\n\n\nWhile we understand that this may result in some dataset duplication, this is not a concern. We are ensuring that the data is fully preserved and accessible.\n\n\n\n\nFinishing Up\n\n\n\n\nIn the Archivers app, make sure to fill out as much information as possible to document your work.\n\n\nCheck the Research checkbox (on the right-hand side) to mark that step as completed.\n\n\nClick \nSave\n.\n\n\nClick \nCheck in URL\n, to release it and allow someone else to work on the next step.\n\n\nYou're done! Move on to the next URL!", 
            "title": "Researching"
        }, 
        {
            "location": "/researching/#what-do-researchers-do", 
            "text": "Researchers review \"uncrawlables\" identified during  Seeding , confirm the URL/dataset is indeed uncrawlable, and investigate how the dataset could be best harvested. Researchers need to have a good understanding of harvesting goals and have some familiarity with datasets.  \n   Recommended Skills     \n  Consider this path if you have strong front-end web experience and enjoy research. An understanding of how federal data is organized (e.g. where \"master\" datasets are) would be valuable.", 
            "title": "What Do Researchers Do?"
        }, 
        {
            "location": "/researching/#getting-set-up-as-a-researcher", 
            "text": "Event organizers (in-person or remote) will tell you how to volunteer for the Researcher role, either through Slack or a form.  As a result, they will send you an invite to the  Archivers app , which helps us coordinate all the data archiving work we do.  Click the invite link, and choose a user name and a password.    Create an account on the DataRefuge Slack using this  slack-in  or use the Slack team recommended by your event organizers. This is where people share expertise and answer each other's questions.   If you need any assistance:  Talk to your DataRescue guide if you are at an in-person event  Or post questions on Slack in the  #general  channel (or other channel recommended by your event organizers).", 
            "title": "Getting Set up as a Researcher"
        }, 
        {
            "location": "/researching/#researchers-and-harvesters", 
            "text": "Researchers and Harvesters should work very closely together as their work will feed from each other and much communication is needed between the two roles.  It may be most effective for Researchers and Harvesters to work together in pairs or small groups. In some cases, a single person might be both a Researcher and a Harvester.  As a Researcher, make sure to check out the  Harvesters documentation  to familiarize yourself with their role.", 
            "title": "Researchers and Harvesters"
        }, 
        {
            "location": "/researching/#claiming-a-dataset-to-research", 
            "text": "Researchers work on datasets that were listed as uncrawlable by Seeders.  Go to the  Archivers app , click  URLS  and then  RESEARCH : all the URLs listed are ready to be researched.  Available URLs are ones that have not been checked out by someone else, i.e. that do not have someone's name in the User column.    Select an available URL and click its UUID to get to the detailed view, then click  Check out this URL . It is now ready for you to work on, and no one else can do anything to it while you have it checked out.  While you go through the research process, make sure to report as much information as possible in the Archivers app, as this is the place were we collectively keep track of all the work done.   \n   Note: URL vs UUID     \n  The  URL  is the link to examine and harvest, and the  UUID  is a canonical ID we use to connect the URL with the data in question. The UUID will have been generated earlier in the process. UUID stands for Universal Unique Identifier.", 
            "title": "Claiming a Dataset to Research"
        }, 
        {
            "location": "/researching/#evaluating-the-data", 
            "text": "Go to the URL, and start inspecting the content.", 
            "title": "Evaluating the Data"
        }, 
        {
            "location": "/researching/#is-the-data-actually-crawlable", 
            "text": "Again, see  EDGI's Guides  for a mostly non-technical introduction to the crawler:   Understanding the Internet Archive Web Crawler  Seeding the Internet Archive\u2019s Web Crawler   Some additional technical notes for answering this:   There is no specific file size cutoff for what is crawlable, but large files should be manually captured anyway.  File types like ZIP, PDF, Excel, etc. are crawlable if they are linked.  The crawler can only follow HTTP links that appear directly in the DOM at load time. (That is, they should appear as  a href ...  tags in the page source.)\nIf links are added by JavaScript or require submitting a form, they are not crawlable.  The crawler does not tolerate web frames (but it is straightforward to inspect a page to obtain the content in the frame directly, and then nominate  that ).  The crawler recently added the ability to crawl FTP, but we will not rely on this; we will treat resources served over FTP as uncrawlable.   YES:  If the URL is crawlable or you locate a crawlable URL that accesses the underlying dataset:   Nominate it using our\n   Chrome extension  or the  bookmarklet .  Click the  Do not harvest  checkbox in the Archivers app.   NO:  If it is confirmed not crawlable:   Search agency websites and data.gov for dataset entry points for your dataset collection.  Tips: Try to understand what datasets are underlying the web pages. Look for related entries in the Archivers app, and ensure that you aren't harvesting a subdirectory if you can harvest the entire directory. Often, data underlying dozens of pages or multiple \"access portal\" apps is also available as one structured data file.  Make note of any better entry point in the  Recommended Approach field, along with any other recommendations on how to proceed with this harvest.    Add other information that could help the Harvester, such as the format (SQL, FTP, ZIP, PDF collections, etc.), approximate size, details about what you found, etc.  Search for related URLs that might already have been listed in the Archivers app that might be covered by the same approach, so as not to duplicate work. You can search for them in the  Link URL  field.   YES and NO:  For example, FTP address, mixed content, big data sets:   Nominate it anyway, but also follow the steps for uncrawlable content above.  While we understand that this may result in some dataset duplication, this is not a concern. We are ensuring that the data is fully preserved and accessible.", 
            "title": "Is the data actually crawlable?"
        }, 
        {
            "location": "/researching/#finishing-up", 
            "text": "In the Archivers app, make sure to fill out as much information as possible to document your work.  Check the Research checkbox (on the right-hand side) to mark that step as completed.  Click  Save .  Click  Check in URL , to release it and allow someone else to work on the next step.  You're done! Move on to the next URL!", 
            "title": "Finishing Up"
        }, 
        {
            "location": "/harvesting/", 
            "text": "What Do Harvesters Do?\n\n\nHarvesters take the \"uncrawlable\" data and try to figure out how to actually capture it based on the recommendations of the Researchers. This is a complex task which can require substantial technical expertise, and which requires different techniques for different tasks.\n\n\n\n  \nRecommended Skills\n \n  \n  Consider this path if you're a skilled technologist with a programming language of your choice (e.g., Python, JavaScript, C, etc.), are comfortable with the command line (bash, shell, powershell), or experience working with structured data. Experience in front-end web development a plus.\n\n\n\n\nImportant Notes\n\n\n\n\n\n\nResearchers and Harvesters\n\n\n\n\nResearchers and Harvesters should work very closely together as their work will feed from each other and much communication is needed between the two roles. It may be most effective for Researchers and Harvesters to work together in pairs or small groups. In some cases, a single person might be both a Researcher and a Harvester.\n\n\nAs a Harvester, make sure to check out the \nResearchers documentation\n to familiarize yourself with their role.\n\n\n\n\n\n\n\n\nThe Notion of \"Meaningful Dataset\"\n\n\n\n\nYour role is to harvest datasets that are complete and \nmeaningful\n. By meaningful we mean: \"will the bag make sense to a scientist\"?\n\n\nFor instance, if a dataset is composed of a spreadsheet without any accompanying key or explanation of what the data represents, it might be completely impossible for a scientist to use it.\n\n\n\n\n\n\n\n\nGetting Set up as a Harvester\n\n\n\n\nSkills recommended for this role: in general, Harvesters need to have some tech skills and a good understanding of harvesting goals.\n\n\nThe organizers of the event (in-person or remote) will tell you how to volunteer for the Harvester role, either through Slack or a form.\n\n\nAs a result, they will send you an invite to the \nArchivers app\n, which helps us coordinate all the data archiving work we do.\n\n\nClick the invite link, and choose a user name and a password.\n\n\n\n\n\n\nCreate an account on the DataRefuge Slack using this \nslack-in\n (or use the Slack team recommended by your event organizers). This is where people share expertise and answer each other's questions. \n\n\nYou might also need other software and utilities set up on your computer, depending on the harvesting methods you use.\n\n\nHarvesters should start by reading this document, which outlines the steps for constructing a proper data archive of the highest possible integrity. The primary focus of this document is on \nsemi-automated harvesting as part of a team\n, and the workflow described is best-suited for volunteers working to preserve small and medium-sized collections. Where possible, we try to link out to other options appropriate to other circumstances.\n\n\nIf you need any assistance:\n\n\nTalk to your DataRescue guide if you are at an in-person event\n\n\nOr post questions on Slack in the #Researchers/Harvesters channel (or other channel recommended by your event organizers).\n\n\n\n\n\n\n\n\n\n\n\n\nHarvesting Toolkit\n\n\nFor in-depth information on tools and techniques to harvest open data, please check EDGI's extensive \ntoolkit\n.\n\n\n1. Claiming a Dataset to Harvest\n\n\n\n\nYou will work on datasets that were confirmed as uncrawlable by Researchers.\n\n\nGo to the \nArchivers app\n, click \nURLS\n and then \nHARVEST\n: all the URLs listed are ready to be harvested.\n\n\nAvailable URLs are the ones that have not been checked out by someone else, i.e. that do not have someone's name in the User column.\n\n\n\n\n\n\nSelect an available URL and click its UUID to get to the detailed view, then click \nCheck out this URL\n. It is now ready for you to work on, and no one else can do anything to it while you have it checked out.\n\n\nWhile you go through the harvesting process, make sure to report as much information as possible in the Archivers app, as this is the place were we collectively keep track of all the work done.\n\n\n\n\n\n  \nNote: URL vs UUID\n \n  \n  The \nURL\n is the link to examine and harvest, and the \nUUID\n is a canonical ID we use to connect the URL with the data in question. The UUID will have been generated earlier in the process. UUID stands for Universal Unique Identifier.\n\n\n\n\n2a. Classify Source Type \n Archivability\n\n\nBefore doing anything, take a minute to understand what you're looking at. It's usually best to do a quick check of the URL to confirm that this data in fact not crawlable. Often as part of the harvesting team, you'll be the first person with a higher level of technical knowledge to review the URL in question.\n\n\nCheck for False-Positives (Content That Is in Fact Crawlable)\n\n\nGenerally, any URL that returns standard HTML, links to more \nHTML mimetype pages\n, and contains little-to-no non-HTML content, is crawlable. \"View source\" from your browser of choice will help see what the crawler itself is seeing. If in fact the data can be crawled, note it in the Google sheet, remove your name from the \"checkout\" column, notify the seeding / crawling team and they will make sure the link is crawled, and move on to another URL.\n\n\nSome Things to Think About While Reviewing a URL\n\n\n\n\nDoes this page use JavaScript to render its content, especially to \ngenerate links\n or \ndynamically pull up images and PDF content\n? Crawlers generally cannot parse dynamically generated content.\n\n\nDoes this URL contain links to non-HTML content? (For example, zip files, PDFs, Excel files, etc...)\n\n\nIs this URL some sort of interface for a large database or service? (For example, an interactive map, API gateway, etc.)\n\n\nDoes this URL contain instructions for connecting to a server, database, or other special source of data?\n\n\n\n\nCheck the Terms of Service!!!\n\n\nBefore you go any further, it is \nalways\n worth confirming that the data in question is in fact open for archiving. If the terms of service explicitly prohibit archiving, \nmake a note of it\n. Generally archive-a-thons are purposely only aimed at publically available data, but it is easy to follow a link away from a publically available source onto a site that has different terms of service.\n\n\nData acquired outside terms of service is not usable.\n\n\n2b. Determine Scale of the Dataset\n\n\nIf the dataset you're looking at is quite large -- say, more than 1000 documents -- capturing it may require more elaborate programming than is described here, and it may be difficult to complete in the timeframe of the event. In that case, you may want to look outside the scope of this document and read the documentation of tools such as the \nEIS WARC archiver\n, which shows how to initiate a larger, fully automated harvest on a web-based virtual machine. Talk to your DataRescue guide to determine how to best proceed.\n\n\n3. Generate HTML, JSON \n Directory\n\n\nTo get started, click \nDownload Zip Starter\n, which will download an empty zip archive structure for the data you are about to harvest.\nThe structure looks like this:\n\n\nDAFD2E80-965F-4989-8A77-843DE716D899\n    \u251c\u2500\u2500 DAFD2E80-965F-4989-8A77-843DE716D899.html\n    \u251c\u2500\u2500 DAFD2E80-965F-4989-8A77-843DE716D899.json\n    \u251c\u2500\u2500 /tools\n    \u2514\u2500\u2500 /data\n\n\n\n\nEach row in the above is:\n\n\nA directory named by the UUID\n    \u251c\u2500\u2500 a .html \nweb archive\n file of the URL for future reference, named with the ID\n    \u251c\u2500\u2500 a .json metadata file that contains relevant metadata, named with the ID\n    \u251c\u2500\u2500 a /tools directory to include any scripts, notes \n files used to acquire the data\n    \u2514\u2500\u2500 a /data directory that contains the data in question\n\n\n\n\nUUID\n\n\nThe goal is to pass this finalized folder off for \n\"bagging\"\n. We repeatedly use the UUID so that we can programmatically work through this data later. It is important that the ID be copied \nexactly\n wherever it appears, with no leading or trailing spaces, and honoring case-sensitivity.\n\n\n[id].html file\n\n\nThe zip starter archive will automatically include a copy of the page corresponding to the URL. The HTML file gives the archive a snapshot of the page at the time of archiving which we can use to monitor for changing data in the future, and corroborate the provenance of the archive itself. We can also use the \n.html\n in conjunction with the scripts you'll include in the tools directory to replicate the archive in the future.\n\n\n\n\n\n[id].json file\n\n\nThe json file is one you'll create by hand to create a machine readable record of the archive. This file contains vital data, including the URL that was archived, and date of archiving. The \nid.json readme\n goes into much more detail.\n\n\n4. Acquire the Data\n\n\nYour method for doing this will depend on the shape and size of the data you're dealing with. A few methods are described below.\n\n\n4a. Identify Data Links \n Acquire Them in a wget Loop\n\n\nIf you encounter a page that links to lots of data (for example a \"downloads\" page), this approach may work well. It's important to only use this approach when you encounter \ndata\n, for example PDF's, .zip archives, .csv datasets, etc.\n\n\nThe tricky part of this approach is generating a list of URLs to download from the page. If you're skilled with using scripts in combination with html-parsers (for example python's wonderful \nbeautiful-soup package\n), go for it. Otherwise, we've included the \njquery-URL-extraction guide\n], which has the advantage of working within a browser and can operate on a page that has been modified by JavaScript.\n\n\nOur example dataset uses jquery-URL, \nleveraging that tool to generate a list of URLs to feed the wget loop\n.\n\n\n4b. Identify Data Links \n Acquire Them via WARCFactory\n\n\nFor search results from large document sets, you may need to do more sophisticated \"scraping\" and \"crawling\" -- again, check out tools built at previous events such as the \nEIS WARC archiver\n or the \nEPA Search Utils\n for ideas on how to proceed.\n\n\n4c. FTP Download\n\n\nGovernment datasets are often stored on FTP. It's pretty easy to crawl these FTP sites with a simple Python script. Have a look at \ndownload_ftp_tree.py\n as an example. Note that the Internet Archive is doing an FTP crawl, so another option (especially if the dataset is large) would be to nominate this as a seed (though FTP seeds should be nominated \nseparately\n from HTTP seeds).\n\n\n4d. API Scrape / Custom Solution\n\n\nIf you encounter an API, chances are you'll have to build some sort of custom solution, or investigate a social angle. For example: asking someone with greater access for a database dump.\n\n\n4e. Automated Full Browser\n\n\nThe last resort of harvesting should be to drive it with a full web browser. It is slower than other approaches such as \nwget\n, \ncURL\n, or a headless browser. Additionally, this implementation is prone to issues where the resulting page is saved before it's done loading. There is a \nruby example\n.\n\n\n5. Write [id].json Metadata \n Add /tools\n\n\nFrom there you'll want to fill out the metadata.json. Use the template below as a guide.\n\n\n\n\nThe json should match the information from the Harvester and use the following format:\n\n\n\n\n{\n \nIndividual source or seed URL\n: \nhttp://www.eia.gov/renewable/data.cfm\n,\n \nUUID\n : \nE30FA3CA-C5CB-41D5-8608-0650D1B6F105\n,\n \nid_agency\n : 2,\n \nid_subagency\n: ,\n \nid_org\n:,\n \nid_suborg\n:,\n \nInstitution facilitating the data capture creation and packaging\n: \nPenn Data Refuge\n,\n \nDate of capture\n: \n2017-01-17\n,\n \nFederal agency data acquired from\n: \nDepartment of Energy/U.S. Energy Information Administration\n,\n \nName of resource\n: \nRenewable and Alternative Fuels\n,\n \nFile formats contained in package\n: \n.pdf, .zip\n,\n \nType(s) of content in package\n: \ndatasets, codebooks\n,\n \nFree text description of capture process\n: \nMetadata was generated by viewing page and using spreadsheet descriptions where necessary, data was bulk downloaded from the page using wget -r on the seed URL and then bagged.\n,\n \nName of package creator\n: \nMallick Hossain and Ben Goldman\n\n }\n\n\n\n\n\n\nMake sure to save this as a .json file.\n\n\n\n\nIn addition, copy any scripts and tools you used into the /tools directory. It may seem strange to copy code multiple times, but this can help later to reconstruct the archiving process for further refinement later on.\n\n\nIt's worth using some judgement here. If a \"script\" you used includes an entire copy of JAVA, or some suite beyond a simple script, it may be better to document your process in a file and leave that in the tools directory instead.\n\n\nTips\n\n\n\n\nIf you encounter a search bar, try entering \"*\" to see if that returns \"all results\".\n\n\nLeave the data unmodified. During the process, you may feel inclined to clean things up, add structure to the data, etc. Avoid temptation. Your finished archive will be hashed so we can compare it later for changes, and it's important that we archive original, unmodified content.\n\n\n\n\n6. Uploading the data\n\n\n\n\nZip all the files pertaining to your dataset within the zip started archive structure and confirm that it is named with the original UUID.\n\n\nUpload the zip file by clicking \nUpload\n in the Archivers app, and selecting \nChoose File\n.\n\n\nNote that files beyond 5 Gigs must be uploaded through the more advanced \nGenerate Upload Token\n option. This will require using the aws command line interface.\n\n\nPlease talk to your DataRescue guide or post on Slack in the Harvesters channel, if you are having issues with this more advanced method.\n\n\n\n\n\n\n\n\n7. Finishing up\n\n\n\n\nIn the Archivers app, make sure to fill out as much information as possible to document your work.\n\n\nCheck the Harvest checkbox (on the right-hand side) to mark that step as completed.\n\n\nClick \nSave\n.\n\n\nClick \nCheck in URL\n, to release it and allow someone else to work on the next step.\n\n\nYou're done! Move on to the next URL!", 
            "title": "Harvesting"
        }, 
        {
            "location": "/harvesting/#what-do-harvesters-do", 
            "text": "Harvesters take the \"uncrawlable\" data and try to figure out how to actually capture it based on the recommendations of the Researchers. This is a complex task which can require substantial technical expertise, and which requires different techniques for different tasks.  \n   Recommended Skills     \n  Consider this path if you're a skilled technologist with a programming language of your choice (e.g., Python, JavaScript, C, etc.), are comfortable with the command line (bash, shell, powershell), or experience working with structured data. Experience in front-end web development a plus.", 
            "title": "What Do Harvesters Do?"
        }, 
        {
            "location": "/harvesting/#important-notes", 
            "text": "Researchers and Harvesters   Researchers and Harvesters should work very closely together as their work will feed from each other and much communication is needed between the two roles. It may be most effective for Researchers and Harvesters to work together in pairs or small groups. In some cases, a single person might be both a Researcher and a Harvester.  As a Harvester, make sure to check out the  Researchers documentation  to familiarize yourself with their role.     The Notion of \"Meaningful Dataset\"   Your role is to harvest datasets that are complete and  meaningful . By meaningful we mean: \"will the bag make sense to a scientist\"?  For instance, if a dataset is composed of a spreadsheet without any accompanying key or explanation of what the data represents, it might be completely impossible for a scientist to use it.", 
            "title": "Important Notes"
        }, 
        {
            "location": "/harvesting/#getting-set-up-as-a-harvester", 
            "text": "Skills recommended for this role: in general, Harvesters need to have some tech skills and a good understanding of harvesting goals.  The organizers of the event (in-person or remote) will tell you how to volunteer for the Harvester role, either through Slack or a form.  As a result, they will send you an invite to the  Archivers app , which helps us coordinate all the data archiving work we do.  Click the invite link, and choose a user name and a password.    Create an account on the DataRefuge Slack using this  slack-in  (or use the Slack team recommended by your event organizers). This is where people share expertise and answer each other's questions.   You might also need other software and utilities set up on your computer, depending on the harvesting methods you use.  Harvesters should start by reading this document, which outlines the steps for constructing a proper data archive of the highest possible integrity. The primary focus of this document is on  semi-automated harvesting as part of a team , and the workflow described is best-suited for volunteers working to preserve small and medium-sized collections. Where possible, we try to link out to other options appropriate to other circumstances.  If you need any assistance:  Talk to your DataRescue guide if you are at an in-person event  Or post questions on Slack in the #Researchers/Harvesters channel (or other channel recommended by your event organizers).", 
            "title": "Getting Set up as a Harvester"
        }, 
        {
            "location": "/harvesting/#harvesting-toolkit", 
            "text": "For in-depth information on tools and techniques to harvest open data, please check EDGI's extensive  toolkit .", 
            "title": "Harvesting Toolkit"
        }, 
        {
            "location": "/harvesting/#1-claiming-a-dataset-to-harvest", 
            "text": "You will work on datasets that were confirmed as uncrawlable by Researchers.  Go to the  Archivers app , click  URLS  and then  HARVEST : all the URLs listed are ready to be harvested.  Available URLs are the ones that have not been checked out by someone else, i.e. that do not have someone's name in the User column.    Select an available URL and click its UUID to get to the detailed view, then click  Check out this URL . It is now ready for you to work on, and no one else can do anything to it while you have it checked out.  While you go through the harvesting process, make sure to report as much information as possible in the Archivers app, as this is the place were we collectively keep track of all the work done.   \n   Note: URL vs UUID     \n  The  URL  is the link to examine and harvest, and the  UUID  is a canonical ID we use to connect the URL with the data in question. The UUID will have been generated earlier in the process. UUID stands for Universal Unique Identifier.", 
            "title": "1. Claiming a Dataset to Harvest"
        }, 
        {
            "location": "/harvesting/#2a-classify-source-type-archivability", 
            "text": "Before doing anything, take a minute to understand what you're looking at. It's usually best to do a quick check of the URL to confirm that this data in fact not crawlable. Often as part of the harvesting team, you'll be the first person with a higher level of technical knowledge to review the URL in question.", 
            "title": "2a. Classify Source Type &amp; Archivability"
        }, 
        {
            "location": "/harvesting/#check-for-false-positives-content-that-is-in-fact-crawlable", 
            "text": "Generally, any URL that returns standard HTML, links to more  HTML mimetype pages , and contains little-to-no non-HTML content, is crawlable. \"View source\" from your browser of choice will help see what the crawler itself is seeing. If in fact the data can be crawled, note it in the Google sheet, remove your name from the \"checkout\" column, notify the seeding / crawling team and they will make sure the link is crawled, and move on to another URL.", 
            "title": "Check for False-Positives (Content That Is in Fact Crawlable)"
        }, 
        {
            "location": "/harvesting/#some-things-to-think-about-while-reviewing-a-url", 
            "text": "Does this page use JavaScript to render its content, especially to  generate links  or  dynamically pull up images and PDF content ? Crawlers generally cannot parse dynamically generated content.  Does this URL contain links to non-HTML content? (For example, zip files, PDFs, Excel files, etc...)  Is this URL some sort of interface for a large database or service? (For example, an interactive map, API gateway, etc.)  Does this URL contain instructions for connecting to a server, database, or other special source of data?", 
            "title": "Some Things to Think About While Reviewing a URL"
        }, 
        {
            "location": "/harvesting/#check-the-terms-of-service", 
            "text": "Before you go any further, it is  always  worth confirming that the data in question is in fact open for archiving. If the terms of service explicitly prohibit archiving,  make a note of it . Generally archive-a-thons are purposely only aimed at publically available data, but it is easy to follow a link away from a publically available source onto a site that has different terms of service.  Data acquired outside terms of service is not usable.", 
            "title": "Check the Terms of Service!!!"
        }, 
        {
            "location": "/harvesting/#2b-determine-scale-of-the-dataset", 
            "text": "If the dataset you're looking at is quite large -- say, more than 1000 documents -- capturing it may require more elaborate programming than is described here, and it may be difficult to complete in the timeframe of the event. In that case, you may want to look outside the scope of this document and read the documentation of tools such as the  EIS WARC archiver , which shows how to initiate a larger, fully automated harvest on a web-based virtual machine. Talk to your DataRescue guide to determine how to best proceed.", 
            "title": "2b. Determine Scale of the Dataset"
        }, 
        {
            "location": "/harvesting/#3-generate-html-json-directory", 
            "text": "To get started, click  Download Zip Starter , which will download an empty zip archive structure for the data you are about to harvest.\nThe structure looks like this:  DAFD2E80-965F-4989-8A77-843DE716D899\n    \u251c\u2500\u2500 DAFD2E80-965F-4989-8A77-843DE716D899.html\n    \u251c\u2500\u2500 DAFD2E80-965F-4989-8A77-843DE716D899.json\n    \u251c\u2500\u2500 /tools\n    \u2514\u2500\u2500 /data  Each row in the above is:  A directory named by the UUID\n    \u251c\u2500\u2500 a .html  web archive  file of the URL for future reference, named with the ID\n    \u251c\u2500\u2500 a .json metadata file that contains relevant metadata, named with the ID\n    \u251c\u2500\u2500 a /tools directory to include any scripts, notes   files used to acquire the data\n    \u2514\u2500\u2500 a /data directory that contains the data in question", 
            "title": "3. Generate HTML, JSON &amp; Directory"
        }, 
        {
            "location": "/harvesting/#uuid", 
            "text": "The goal is to pass this finalized folder off for  \"bagging\" . We repeatedly use the UUID so that we can programmatically work through this data later. It is important that the ID be copied  exactly  wherever it appears, with no leading or trailing spaces, and honoring case-sensitivity.", 
            "title": "UUID"
        }, 
        {
            "location": "/harvesting/#idhtml-file", 
            "text": "The zip starter archive will automatically include a copy of the page corresponding to the URL. The HTML file gives the archive a snapshot of the page at the time of archiving which we can use to monitor for changing data in the future, and corroborate the provenance of the archive itself. We can also use the  .html  in conjunction with the scripts you'll include in the tools directory to replicate the archive in the future.", 
            "title": "[id].html file"
        }, 
        {
            "location": "/harvesting/#idjson-file", 
            "text": "The json file is one you'll create by hand to create a machine readable record of the archive. This file contains vital data, including the URL that was archived, and date of archiving. The  id.json readme  goes into much more detail.", 
            "title": "[id].json file"
        }, 
        {
            "location": "/harvesting/#4-acquire-the-data", 
            "text": "Your method for doing this will depend on the shape and size of the data you're dealing with. A few methods are described below.", 
            "title": "4. Acquire the Data"
        }, 
        {
            "location": "/harvesting/#4a-identify-data-links-acquire-them-in-a-wget-loop", 
            "text": "If you encounter a page that links to lots of data (for example a \"downloads\" page), this approach may work well. It's important to only use this approach when you encounter  data , for example PDF's, .zip archives, .csv datasets, etc.  The tricky part of this approach is generating a list of URLs to download from the page. If you're skilled with using scripts in combination with html-parsers (for example python's wonderful  beautiful-soup package ), go for it. Otherwise, we've included the  jquery-URL-extraction guide ], which has the advantage of working within a browser and can operate on a page that has been modified by JavaScript.  Our example dataset uses jquery-URL,  leveraging that tool to generate a list of URLs to feed the wget loop .", 
            "title": "4a. Identify Data Links &amp; Acquire Them in a wget Loop"
        }, 
        {
            "location": "/harvesting/#4b-identify-data-links-acquire-them-via-warcfactory", 
            "text": "For search results from large document sets, you may need to do more sophisticated \"scraping\" and \"crawling\" -- again, check out tools built at previous events such as the  EIS WARC archiver  or the  EPA Search Utils  for ideas on how to proceed.", 
            "title": "4b. Identify Data Links &amp; Acquire Them via WARCFactory"
        }, 
        {
            "location": "/harvesting/#4c-ftp-download", 
            "text": "Government datasets are often stored on FTP. It's pretty easy to crawl these FTP sites with a simple Python script. Have a look at  download_ftp_tree.py  as an example. Note that the Internet Archive is doing an FTP crawl, so another option (especially if the dataset is large) would be to nominate this as a seed (though FTP seeds should be nominated  separately  from HTTP seeds).", 
            "title": "4c. FTP Download"
        }, 
        {
            "location": "/harvesting/#4d-api-scrape-custom-solution", 
            "text": "If you encounter an API, chances are you'll have to build some sort of custom solution, or investigate a social angle. For example: asking someone with greater access for a database dump.", 
            "title": "4d. API Scrape / Custom Solution"
        }, 
        {
            "location": "/harvesting/#4e-automated-full-browser", 
            "text": "The last resort of harvesting should be to drive it with a full web browser. It is slower than other approaches such as  wget ,  cURL , or a headless browser. Additionally, this implementation is prone to issues where the resulting page is saved before it's done loading. There is a  ruby example .", 
            "title": "4e. Automated Full Browser"
        }, 
        {
            "location": "/harvesting/#5-write-idjson-metadata-add-tools", 
            "text": "From there you'll want to fill out the metadata.json. Use the template below as a guide.   The json should match the information from the Harvester and use the following format:   {\n  Individual source or seed URL :  http://www.eia.gov/renewable/data.cfm ,\n  UUID  :  E30FA3CA-C5CB-41D5-8608-0650D1B6F105 ,\n  id_agency  : 2,\n  id_subagency : ,\n  id_org :,\n  id_suborg :,\n  Institution facilitating the data capture creation and packaging :  Penn Data Refuge ,\n  Date of capture :  2017-01-17 ,\n  Federal agency data acquired from :  Department of Energy/U.S. Energy Information Administration ,\n  Name of resource :  Renewable and Alternative Fuels ,\n  File formats contained in package :  .pdf, .zip ,\n  Type(s) of content in package :  datasets, codebooks ,\n  Free text description of capture process :  Metadata was generated by viewing page and using spreadsheet descriptions where necessary, data was bulk downloaded from the page using wget -r on the seed URL and then bagged. ,\n  Name of package creator :  Mallick Hossain and Ben Goldman \n }   Make sure to save this as a .json file.   In addition, copy any scripts and tools you used into the /tools directory. It may seem strange to copy code multiple times, but this can help later to reconstruct the archiving process for further refinement later on.  It's worth using some judgement here. If a \"script\" you used includes an entire copy of JAVA, or some suite beyond a simple script, it may be better to document your process in a file and leave that in the tools directory instead.", 
            "title": "5. Write [id].json Metadata &amp; Add /tools"
        }, 
        {
            "location": "/harvesting/#tips", 
            "text": "If you encounter a search bar, try entering \"*\" to see if that returns \"all results\".  Leave the data unmodified. During the process, you may feel inclined to clean things up, add structure to the data, etc. Avoid temptation. Your finished archive will be hashed so we can compare it later for changes, and it's important that we archive original, unmodified content.", 
            "title": "Tips"
        }, 
        {
            "location": "/harvesting/#6-uploading-the-data", 
            "text": "Zip all the files pertaining to your dataset within the zip started archive structure and confirm that it is named with the original UUID.  Upload the zip file by clicking  Upload  in the Archivers app, and selecting  Choose File .  Note that files beyond 5 Gigs must be uploaded through the more advanced  Generate Upload Token  option. This will require using the aws command line interface.  Please talk to your DataRescue guide or post on Slack in the Harvesters channel, if you are having issues with this more advanced method.", 
            "title": "6. Uploading the data"
        }, 
        {
            "location": "/harvesting/#7-finishing-up", 
            "text": "In the Archivers app, make sure to fill out as much information as possible to document your work.  Check the Harvest checkbox (on the right-hand side) to mark that step as completed.  Click  Save .  Click  Check in URL , to release it and allow someone else to work on the next step.  You're done! Move on to the next URL!", 
            "title": "7. Finishing up"
        }, 
        {
            "location": "/bagging/", 
            "text": "What Do Baggers Do?\n\n\nBaggers do some quality assurance on the dataset to make sure the content is correct and corresponds to what was described in the spreadsheet. Then they package the data into a bagit file (or \"bag\"), which includes basic technical metadata, and upload it to the final DataRefuge destination.\n\n\n\n  \nRecommended Skills\n \n  \n  Consider this path if you have data or web archiving experience, or have strong tech skills and an attention to detail.\n\n\n\n\nGetting Set up as a Bagger\n\n\n\n\nSkills recommended for this role: in general, Baggers need to have strong tech skills and a good understanding of harvesting goals.\n\n\nApply to become a Bagger by filling out \nthis form\n\n\nNote that an email address is required to apply.\n\n\nNote also that you should be willing to have your real name be associated with the datasets, to follow archival best practices (see \nguidelines on archival best practices for Data Refuge\n for more information).\n\n\n\n\n\n\nThe organizers of the event (in-person or remote) will send you an invite to the \nArchivers app\n, which helps us coordinate all the data archiving work we do.\n\n\nClick the invite link, and choose a user name and a password.\n\n\n\n\n\n\nCreate an account on the DataRefuge Slack using this \nslack-in\n (or use the Slack team recommended by your event organizers). This is where people share expertise and answer each other's questions.   \n\n\nGet set up with Python and the Python script to make a bag at the command line: \nhttps://github.com/LibraryOfCongress/bagit-python\n\n\nIf you need any assistance:\n\n\nTalk to your DataRescue guide if you are at an in-person event.\n\n\nOr post questions on Slack in the #Baggers channel (or other channel recommended by your event organizers).\n\n\n\n\n\n\n\n\nClaiming a Dataset for Bagging\n\n\n\n\nYou will work on datasets that were harvested by Harvesters.\n\n\nGo to the \nArchivers app\n, click \nURLS\n and then \nBAG\n: all the URLs listed are ready to be bagged.\n\n\nAvailable URLs are the ones that have not been checked out by someone else, i.e. that do not have someone's name in the User column.\n\n\n\n\n\n\nSelect an available URL and click its UUID to get to the detailed view, then click \nCheck out this URL\n. It is now ready for you to work on, and no one else can do anything to it while you have it checked out.\n\n\nWhile you go through the bagging process, make sure to report as much information as possible in the Archivers app, as this is the place where we collectively keep track of all the work done.\n\n\n\n\n\n  \nNote: URL vs UUID\n \n  \n  The \nURL\n is the link to examine and harvest, and the \nUUID\n is a canonical ID we use to connect the URL with the data in question. The UUID will have been generated earlier in the process. UUID stands for Universal Unique Identifier.\n\n\n\n\nDownloading \n Opening the Dataset\n\n\n\n\nThe zipped dataset that is ready to be bagged is under \nHarvest Url / Location\n in the the Archivers app. Download it to your laptop and unzip it.\n\n\nExtra check: Is this URL truly ready to bag?\n\n\nWhile everybody is doing their best to provide accurate information, occasionally a URL will be presented as \"ready to bag\" but, in fact, is not. Symptoms include:\n\n\nThere is no value in the \"Harvest Url / Location\" field.\n\n\nThere is a note in the Harvest section that seems to indicate that the harvest was only partially performed.  \n\n\nIn either case, uncheck the \"Harvest\" checkbox, and add a note in the \nNotes From Harvest\n field indicating that the URL does not seem ready for bagging and needs to be reviewed by a Harvester.\n\n\n\n\n\n\n\n\n\n\n\n\nQuality Assurance\n\n\n\n\nConfirm the harvested files:\n\n\nGo to the original URL and check that the dataset is complete and accurate.\n\n\nYou also need to check that the dataset is meaningful, that is: \"will the bag make sense to a scientist\"?\nFor instance, if a dataset is composed of a spreadsheet without any accompanying key or explanation of what the data represents, it might be completely impossible for a scientist to use it.\n\n\nSpot-check to make sure the files open properly and are not faulty in any way.\n\n\n\n\n\n\nConfirm contents of JSON file:\n\n\nThe JSON should match the information from the Harvester and use the following format:  \n\n\n\n\n\n\n\n\n{\n  \nIndividual source or seed URL\n: \nhttp://www.eia.gov/renewable/data.cfm\n,\n  \nUUID\n : \nE30FA3CA-C5CB-41D5-8608-0650D1B6F105\n,\n  \nid_agency\n : 2,\n  \nid_subagency\n: ,\n  \nid_org\n:,\n  \nid_suborg\n:,\n  \nInstitution facilitating the data capture creation and packaging\n: \nPenn Data Refuge\n,\n  \nDate of capture\n: \n2017-01-17\n,\n  \nFederal agency data acquired from\n: \nDepartment of Energy/U.S. Energy Information Administration\n,\n  \nName of resource\n: \nRenewable and Alternative Fuels\n,\n  \nFile formats contained in package\n: \n.pdf, .zip\n,\n  \nType(s) of content in package\n: \ndatasets, codebooks\n,\n  \nFree text description of capture process\n: \nMetadata was generated by viewing page and using spreadsheet descriptions where necessary, data was bulk downloaded from the page using wget -r on the seed URL and then bagged.\n,\n  \nName of package creator\n: \nMallick Hossain and Ben Goldman\n\n}\n\n\n\n\n\n\nIf you make any changes, make sure to save this as a .json file.\n\n\nConfirm that the JSON file is within the package with the dataset(s).\n\n\n\n\nCreating the Bag\n\n\n\n\nRun the python command line script that creates the bag:\n\n\n\n\nbagit.py --contact-name '[your name]' /directory/to/bag\n\n\n\n\n\n\nYou should be left with a 'data' folder (which contains the downloaded content and metadata file) and four separate bagit files:\n\n\nbag-info.txt\n\n\nbagit.txt\n\n\nmanifest-md5.txt\n\n\ntagmanifest-md5.txt\n\n\n\n\n\n\nIMPORTANT: It's crucial that you do not move or open the bag once you have created it. This may create hidden files that could make the bag invalid later.\n\n\nRun the following python command line script to do an initial validation of a bag:\n\n\n\n\nbagit.py --validate [directory/of/bag/to/validate]\n\n\n\n\n\n\nIf it comes back as valid, proceed to the next step of creating the zip file and uploading it. If it does not, make a note of the error, review your steps, and re-bag the file. If you continue to get invalid bags, please see a DataRescue guide or reach out in the Baggers Slack channel.\n\n\n\n\nCreating the Zip File and Uploading It\n\n\n\n\nZip this entire collection (data folder and bagit files) and confirm that it is named with the row's UUID.\n\n\nWithout moving the file\n, upload the zipped bag using the application http://drp-upload-bagger.herokuapp.com/ using the user ID and password provided in the Archivers App\n\n\nMake sure to select the name of your event in the dropdown (and \"remote\" if you are working remotely)\n\n\nThe application will return the location URL for your zip file.\n\n\nThe syntax will be \n[UrlStub]/[UUID].zip\n\n\nCut and paste that URL to the \nBag URL\n field in the Archivers app.\n\n\nNote that files beyond 5 Gigs must be uploaded through the more advanced \nGenerate Upload Token\n option. This will require using the aws command line interface.\n\n\nPlease talk to your DataRescue guide or post on Slack in the Baggers channel, if you are having issues with this more advanced method.\n\n\n\n\nQuality Assurance and Finishing Up\n\n\n\n\nOnce the zip file has been fully uploaded, download the bag back to your computer (use the URL provided by the Archiver App) and run the following python command line script for validation:\n\n\n\n\nbagit.py --validate [directory/of/bag/to/validate]\n\n\n\n\n\n\nIf it comes back as valid, open the bag and spot-check to make sure everything looks the same as when you uploaded it (this will not affect the validity of the bags already uploaded). If all seems right, proceed to the rest of the quality assurance steps. If it does not come back as valid, make a note of the error, review your steps, and re-bag the file. If you continue to get invalid bags, please see a DataRescue guide or reach out in the Baggers Slack channel.\n\n\nFill out as much information as possible in the \nNotes From Bagging\n field in the Archivers app to document your work.\n\n\nCheck the checkbox that certifies this is a \"well-checked bag\".\n\n\nCheck the Bag checkbox (on the right-hand side) to mark that step as completed.\n\n\nClick \nSave\n.\n\n\nClick \nCheck in URL\n to release it and allow someone else to work on the next step.", 
            "title": "Bagging"
        }, 
        {
            "location": "/bagging/#what-do-baggers-do", 
            "text": "Baggers do some quality assurance on the dataset to make sure the content is correct and corresponds to what was described in the spreadsheet. Then they package the data into a bagit file (or \"bag\"), which includes basic technical metadata, and upload it to the final DataRefuge destination.  \n   Recommended Skills     \n  Consider this path if you have data or web archiving experience, or have strong tech skills and an attention to detail.", 
            "title": "What Do Baggers Do?"
        }, 
        {
            "location": "/bagging/#getting-set-up-as-a-bagger", 
            "text": "Skills recommended for this role: in general, Baggers need to have strong tech skills and a good understanding of harvesting goals.  Apply to become a Bagger by filling out  this form  Note that an email address is required to apply.  Note also that you should be willing to have your real name be associated with the datasets, to follow archival best practices (see  guidelines on archival best practices for Data Refuge  for more information).    The organizers of the event (in-person or remote) will send you an invite to the  Archivers app , which helps us coordinate all the data archiving work we do.  Click the invite link, and choose a user name and a password.    Create an account on the DataRefuge Slack using this  slack-in  (or use the Slack team recommended by your event organizers). This is where people share expertise and answer each other's questions.     Get set up with Python and the Python script to make a bag at the command line:  https://github.com/LibraryOfCongress/bagit-python  If you need any assistance:  Talk to your DataRescue guide if you are at an in-person event.  Or post questions on Slack in the #Baggers channel (or other channel recommended by your event organizers).", 
            "title": "Getting Set up as a Bagger"
        }, 
        {
            "location": "/bagging/#claiming-a-dataset-for-bagging", 
            "text": "You will work on datasets that were harvested by Harvesters.  Go to the  Archivers app , click  URLS  and then  BAG : all the URLs listed are ready to be bagged.  Available URLs are the ones that have not been checked out by someone else, i.e. that do not have someone's name in the User column.    Select an available URL and click its UUID to get to the detailed view, then click  Check out this URL . It is now ready for you to work on, and no one else can do anything to it while you have it checked out.  While you go through the bagging process, make sure to report as much information as possible in the Archivers app, as this is the place where we collectively keep track of all the work done.   \n   Note: URL vs UUID     \n  The  URL  is the link to examine and harvest, and the  UUID  is a canonical ID we use to connect the URL with the data in question. The UUID will have been generated earlier in the process. UUID stands for Universal Unique Identifier.", 
            "title": "Claiming a Dataset for Bagging"
        }, 
        {
            "location": "/bagging/#downloading-opening-the-dataset", 
            "text": "The zipped dataset that is ready to be bagged is under  Harvest Url / Location  in the the Archivers app. Download it to your laptop and unzip it.  Extra check: Is this URL truly ready to bag?  While everybody is doing their best to provide accurate information, occasionally a URL will be presented as \"ready to bag\" but, in fact, is not. Symptoms include:  There is no value in the \"Harvest Url / Location\" field.  There is a note in the Harvest section that seems to indicate that the harvest was only partially performed.    In either case, uncheck the \"Harvest\" checkbox, and add a note in the  Notes From Harvest  field indicating that the URL does not seem ready for bagging and needs to be reviewed by a Harvester.", 
            "title": "Downloading &amp; Opening the Dataset"
        }, 
        {
            "location": "/bagging/#quality-assurance", 
            "text": "Confirm the harvested files:  Go to the original URL and check that the dataset is complete and accurate.  You also need to check that the dataset is meaningful, that is: \"will the bag make sense to a scientist\"?\nFor instance, if a dataset is composed of a spreadsheet without any accompanying key or explanation of what the data represents, it might be completely impossible for a scientist to use it.  Spot-check to make sure the files open properly and are not faulty in any way.    Confirm contents of JSON file:  The JSON should match the information from the Harvester and use the following format:       {\n   Individual source or seed URL :  http://www.eia.gov/renewable/data.cfm ,\n   UUID  :  E30FA3CA-C5CB-41D5-8608-0650D1B6F105 ,\n   id_agency  : 2,\n   id_subagency : ,\n   id_org :,\n   id_suborg :,\n   Institution facilitating the data capture creation and packaging :  Penn Data Refuge ,\n   Date of capture :  2017-01-17 ,\n   Federal agency data acquired from :  Department of Energy/U.S. Energy Information Administration ,\n   Name of resource :  Renewable and Alternative Fuels ,\n   File formats contained in package :  .pdf, .zip ,\n   Type(s) of content in package :  datasets, codebooks ,\n   Free text description of capture process :  Metadata was generated by viewing page and using spreadsheet descriptions where necessary, data was bulk downloaded from the page using wget -r on the seed URL and then bagged. ,\n   Name of package creator :  Mallick Hossain and Ben Goldman \n}   If you make any changes, make sure to save this as a .json file.  Confirm that the JSON file is within the package with the dataset(s).", 
            "title": "Quality Assurance"
        }, 
        {
            "location": "/bagging/#creating-the-bag", 
            "text": "Run the python command line script that creates the bag:   bagit.py --contact-name '[your name]' /directory/to/bag   You should be left with a 'data' folder (which contains the downloaded content and metadata file) and four separate bagit files:  bag-info.txt  bagit.txt  manifest-md5.txt  tagmanifest-md5.txt    IMPORTANT: It's crucial that you do not move or open the bag once you have created it. This may create hidden files that could make the bag invalid later.  Run the following python command line script to do an initial validation of a bag:   bagit.py --validate [directory/of/bag/to/validate]   If it comes back as valid, proceed to the next step of creating the zip file and uploading it. If it does not, make a note of the error, review your steps, and re-bag the file. If you continue to get invalid bags, please see a DataRescue guide or reach out in the Baggers Slack channel.", 
            "title": "Creating the Bag"
        }, 
        {
            "location": "/bagging/#creating-the-zip-file-and-uploading-it", 
            "text": "Zip this entire collection (data folder and bagit files) and confirm that it is named with the row's UUID.  Without moving the file , upload the zipped bag using the application http://drp-upload-bagger.herokuapp.com/ using the user ID and password provided in the Archivers App  Make sure to select the name of your event in the dropdown (and \"remote\" if you are working remotely)  The application will return the location URL for your zip file.  The syntax will be  [UrlStub]/[UUID].zip  Cut and paste that URL to the  Bag URL  field in the Archivers app.  Note that files beyond 5 Gigs must be uploaded through the more advanced  Generate Upload Token  option. This will require using the aws command line interface.  Please talk to your DataRescue guide or post on Slack in the Baggers channel, if you are having issues with this more advanced method.", 
            "title": "Creating the Zip File and Uploading It"
        }, 
        {
            "location": "/bagging/#quality-assurance-and-finishing-up", 
            "text": "Once the zip file has been fully uploaded, download the bag back to your computer (use the URL provided by the Archiver App) and run the following python command line script for validation:   bagit.py --validate [directory/of/bag/to/validate]   If it comes back as valid, open the bag and spot-check to make sure everything looks the same as when you uploaded it (this will not affect the validity of the bags already uploaded). If all seems right, proceed to the rest of the quality assurance steps. If it does not come back as valid, make a note of the error, review your steps, and re-bag the file. If you continue to get invalid bags, please see a DataRescue guide or reach out in the Baggers Slack channel.  Fill out as much information as possible in the  Notes From Bagging  field in the Archivers app to document your work.  Check the checkbox that certifies this is a \"well-checked bag\".  Check the Bag checkbox (on the right-hand side) to mark that step as completed.  Click  Save .  Click  Check in URL  to release it and allow someone else to work on the next step.", 
            "title": "Quality Assurance and Finishing Up"
        }, 
        {
            "location": "/describing/", 
            "text": "What Do Describers Do?\n\n\nDescribers create a descriptive record in the DataRefuge CKAN repository for each bag. Then they link the record to the bag, and make the record public.\n\n\n\n  \nRecommended Skills\n \n  \n  Consider this path if you have experience working with scientific data (particularly climate or environmental data) or with creating metadata.\n\n\n\n\nGetting Set up as a Describer\n\n\n\n\nSkills recommended for this role: in general, Describers need to have a good handle on metadata practices.\n\n\nApply to become a Describer by asking your DataRescue guide or by filling out \nthis form\n.\n\n\nNote that an email address is required to apply.\n\n\nNote also that you should be willing to have your real name be associated with the datasets, to follow archival best practices (see \nguidelines on archival best practices for Data Refuge\n for more information).\n\n\n\n\n\n\nThe organizers of the event (in-person or remote) will send you an invite to the \nArchivers app\n, which helps us coordinate all the data archiving work we do.\n\n\nClick the invite link, and choose a user name and a password.\n\n\n\n\n\n\nCreate an account on the DataRefuge Slack using this \nslack-in\n (or use the Slack team recommended by your event organizers). This is where people share expertise and answer each other's questions.\n\n\nAsk your event organizer to send you an invite.\n\n\n\n\n\n\nThe organizers will also create an account for you in the CKAN instance at https://www.datarefuge.org/.\n\n\nTest that you can log in successfully.\n\n\n\n\n\n\nGet set up with Python and the Python script to make a bag at the command line: https://github.com/LibraryOfCongress/bagit-python\n\n\nIf you need any assistance:\n\n\nTalk to your DataRescue guide if you are at an in-person event.\n\n\nOr post questions on Slack in the Describers channel (or other channel recommended by your event organizers).\n\n\n\n\n\n\n\n\nClaiming a Bag\n\n\n\n\nYou will work on datasets that were bagged by Baggers.\n\n\nGo to the \nArchivers app\n, click \nURLS\n and then \nDESCRIBE\n: all the URLs listed are ready to be added to the CKAN instance.\n\n\nAvailable URLs are ones that have not been checked out by someone else, i.e. that do not have someone's name in the User column.\n\n\n\n\n\n\nSelect an available URL and click its UUID to get to the detailed view, then click \nCheck out this URL\n. It is now ready for you to work on, and no one else can do anything to it while you have it checked out.\n\n\n\n\n\n  \nNote: URL vs UUID\n \n  \n  The \nURL\n is the link to examine and harvest, and the \nUUID\n is a canonical ID we use to connect the URL with the data in question. The UUID will have been generated earlier in the process. UUID stands for Universal Unique Identifier.\n\n\n\n\nQA Step\n\n\n\n\nIn the Archivers app, scroll down to the \nDescribe\n section.\n\n\nThe URL of the zipped bag is in the \nBag Url / Location\n field.\n\n\nCut and paste that URL into your browser and download it. \n\n\nAfter downloading, unzip it.\n\n\nSpot-check some of the files (make sure they open and look normal, i.e., not garbled).\n\n\nIf the file fails QA:\n\n\nUncheck the Bagging checkbox.\n\n\nMake a note in the \nNotes From Bagging\n field, explaining in what way the bag failed QA and asking a bagger to please fix the issue.\n\n\n\n\n\n\n\n\nCreate New Record in CKAN\n\n\n\n\nGo to \nCKAN\n and click Organizations in the top menu.\n\n\nChoose the organization (i.e., federal agency) that your dataset belongs to, e.g. \nNOAA\n, and click it.\n\n\nIf the Organization you need does not exist yet, create it by clicking \nAdd Organization\n.\n\n\n\n\n\n\nClick \"Add Dataset\".\n\n\nStart entering metadata in the new record, following the metadata template below:\n\n\nTitle:\n Title of dataset, e.g., \"Form EIA-411 Data\".\n\n\nDescription:\n Usually copied and pasted description found on webpage.\n\n\nTags:\n Basic descriptive keywords, e.g., \"electric reliability\", \"electricity\", \"power systems\".\n\n\nLicense:\n Choose value in dropdown. If there is no indicated license, select \"Other - Public Domain\".\n\n\nOrganization:\n Choose value in dropdown, e.g., \"United States Department of Energy\".\n\n\nVisibility:\n Select \"Public\".\n\n\nSource:\n URL where site is live, also in JSON, e.g. \"http://www.eia.gov/electricity/data/eia411/\".\n\n\n\n\n\n\nTo decide what value to enter in each field:\n\n\nOpen the JSON file that is in the bag you have downloaded; it contains some of the metadata you need.\n\n\nGo to the original location of the item on the federal agency website (found in the JSON file), to find more facts about the item such as description, title of the dataset, etc.\n\n\nAlternatively, you can also open the HTML file that should be included in the bag and is a copy of that original main page.\n\n\n\n\n\n\n\n\n\n\n\n\nEnhancing Existing Metadata\n\n\nThese sites will help you obtain federally-sourced metadata that can be added to the CKAN record for more accurate metadata:\n\n\n\n\nEPA:\n\n\nhttps://www.epa.gov/enviro/facility-registry-service-frs\n\n\nhttps://edg.epa.gov/metadata/catalog/main/home.page\n\n\n\n\n\n\n\n\nThese sites are sources of scientific metadata standards to review when choosing keywords:\n\n\n\n\nGCMD Keywords:\n\n\nhttps://wiki.earthdata.nasa.gov/display/cmr/gcmd+keyword+access\n - Downloadable CSV files of the GCMD taxonomies.\n\n\n\n\n\n\nATRAC:\n\n\nhttps://www.ncdc.noaa.gov/atrac/index.html\n - This is a free tool to give access to geographic metadata standards including autopopulating thesauri (GCMD and others commonly used with climate data).\n\n\n\n\n\n\n\n\nLinking the CKAN Record to the Bag\n\n\n\n\nClick \"Next: Add Data\" at the bottom of the CKAN form.\n\n\nEnter the following information:\n\n\nLink:\n Bag URL, e.g., \nhttps://drp-upload-bagger.s3.amazonaws.com/remote/77DD634E-EBCE-412E-88B5-A02B0EF12AF6_2.zip\n.\n\n\nName:\n filename, e.g., \n77DD634E-EBCE-412E-88B5-A02B0EF12AF6_2.zip\n.\n\n\nFormat:\n select \"Zip\".\n\n\n\n\n\n\nClick \"Finish\".\n\n\nTest that the link you just created works by clicking it, and verifying that the file begins to download.\n\n\nNote that you don't need to finish downloading it again.\n\n\n\n\n\n\n\n\nFinishing Up\n\n\n\n\nIn the Archivers app, add the URL to the CKAN record in the \nCKAN URL\n field.\n\n\nThe syntax will be:\n \nhttps://www.datarefuge.org//dataset/[datasetNameGeneratedByCkan]\n\n\n\n\n\n\nAdd any useful notes to document your work.\n\n\nCheck the Describe checkbox (on the right-hand side) to mark that step as completed.\n\n\nClick \nSave\n.\n\n\nClick \nCheck in URL\n, to release it.\n\n\n\n\nPossible Tools: JSON Viewers\n\n\n\n\njsoneditoronline.org\n\n\njsonviewer.stack.hu", 
            "title": "Describing"
        }, 
        {
            "location": "/describing/#what-do-describers-do", 
            "text": "Describers create a descriptive record in the DataRefuge CKAN repository for each bag. Then they link the record to the bag, and make the record public.  \n   Recommended Skills     \n  Consider this path if you have experience working with scientific data (particularly climate or environmental data) or with creating metadata.", 
            "title": "What Do Describers Do?"
        }, 
        {
            "location": "/describing/#getting-set-up-as-a-describer", 
            "text": "Skills recommended for this role: in general, Describers need to have a good handle on metadata practices.  Apply to become a Describer by asking your DataRescue guide or by filling out  this form .  Note that an email address is required to apply.  Note also that you should be willing to have your real name be associated with the datasets, to follow archival best practices (see  guidelines on archival best practices for Data Refuge  for more information).    The organizers of the event (in-person or remote) will send you an invite to the  Archivers app , which helps us coordinate all the data archiving work we do.  Click the invite link, and choose a user name and a password.    Create an account on the DataRefuge Slack using this  slack-in  (or use the Slack team recommended by your event organizers). This is where people share expertise and answer each other's questions.  Ask your event organizer to send you an invite.    The organizers will also create an account for you in the CKAN instance at https://www.datarefuge.org/.  Test that you can log in successfully.    Get set up with Python and the Python script to make a bag at the command line: https://github.com/LibraryOfCongress/bagit-python  If you need any assistance:  Talk to your DataRescue guide if you are at an in-person event.  Or post questions on Slack in the Describers channel (or other channel recommended by your event organizers).", 
            "title": "Getting Set up as a Describer"
        }, 
        {
            "location": "/describing/#claiming-a-bag", 
            "text": "You will work on datasets that were bagged by Baggers.  Go to the  Archivers app , click  URLS  and then  DESCRIBE : all the URLs listed are ready to be added to the CKAN instance.  Available URLs are ones that have not been checked out by someone else, i.e. that do not have someone's name in the User column.    Select an available URL and click its UUID to get to the detailed view, then click  Check out this URL . It is now ready for you to work on, and no one else can do anything to it while you have it checked out.   \n   Note: URL vs UUID     \n  The  URL  is the link to examine and harvest, and the  UUID  is a canonical ID we use to connect the URL with the data in question. The UUID will have been generated earlier in the process. UUID stands for Universal Unique Identifier.", 
            "title": "Claiming a Bag"
        }, 
        {
            "location": "/describing/#qa-step", 
            "text": "In the Archivers app, scroll down to the  Describe  section.  The URL of the zipped bag is in the  Bag Url / Location  field.  Cut and paste that URL into your browser and download it.   After downloading, unzip it.  Spot-check some of the files (make sure they open and look normal, i.e., not garbled).  If the file fails QA:  Uncheck the Bagging checkbox.  Make a note in the  Notes From Bagging  field, explaining in what way the bag failed QA and asking a bagger to please fix the issue.", 
            "title": "QA Step"
        }, 
        {
            "location": "/describing/#create-new-record-in-ckan", 
            "text": "Go to  CKAN  and click Organizations in the top menu.  Choose the organization (i.e., federal agency) that your dataset belongs to, e.g.  NOAA , and click it.  If the Organization you need does not exist yet, create it by clicking  Add Organization .    Click \"Add Dataset\".  Start entering metadata in the new record, following the metadata template below:  Title:  Title of dataset, e.g., \"Form EIA-411 Data\".  Description:  Usually copied and pasted description found on webpage.  Tags:  Basic descriptive keywords, e.g., \"electric reliability\", \"electricity\", \"power systems\".  License:  Choose value in dropdown. If there is no indicated license, select \"Other - Public Domain\".  Organization:  Choose value in dropdown, e.g., \"United States Department of Energy\".  Visibility:  Select \"Public\".  Source:  URL where site is live, also in JSON, e.g. \"http://www.eia.gov/electricity/data/eia411/\".    To decide what value to enter in each field:  Open the JSON file that is in the bag you have downloaded; it contains some of the metadata you need.  Go to the original location of the item on the federal agency website (found in the JSON file), to find more facts about the item such as description, title of the dataset, etc.  Alternatively, you can also open the HTML file that should be included in the bag and is a copy of that original main page.", 
            "title": "Create New Record in CKAN"
        }, 
        {
            "location": "/describing/#enhancing-existing-metadata", 
            "text": "These sites will help you obtain federally-sourced metadata that can be added to the CKAN record for more accurate metadata:   EPA:  https://www.epa.gov/enviro/facility-registry-service-frs  https://edg.epa.gov/metadata/catalog/main/home.page     These sites are sources of scientific metadata standards to review when choosing keywords:   GCMD Keywords:  https://wiki.earthdata.nasa.gov/display/cmr/gcmd+keyword+access  - Downloadable CSV files of the GCMD taxonomies.    ATRAC:  https://www.ncdc.noaa.gov/atrac/index.html  - This is a free tool to give access to geographic metadata standards including autopopulating thesauri (GCMD and others commonly used with climate data).", 
            "title": "Enhancing Existing Metadata"
        }, 
        {
            "location": "/describing/#linking-the-ckan-record-to-the-bag", 
            "text": "Click \"Next: Add Data\" at the bottom of the CKAN form.  Enter the following information:  Link:  Bag URL, e.g.,  https://drp-upload-bagger.s3.amazonaws.com/remote/77DD634E-EBCE-412E-88B5-A02B0EF12AF6_2.zip .  Name:  filename, e.g.,  77DD634E-EBCE-412E-88B5-A02B0EF12AF6_2.zip .  Format:  select \"Zip\".    Click \"Finish\".  Test that the link you just created works by clicking it, and verifying that the file begins to download.  Note that you don't need to finish downloading it again.", 
            "title": "Linking the CKAN Record to the Bag"
        }, 
        {
            "location": "/describing/#finishing-up", 
            "text": "In the Archivers app, add the URL to the CKAN record in the  CKAN URL  field.  The syntax will be:\n  https://www.datarefuge.org//dataset/[datasetNameGeneratedByCkan]    Add any useful notes to document your work.  Check the Describe checkbox (on the right-hand side) to mark that step as completed.  Click  Save .  Click  Check in URL , to release it.", 
            "title": "Finishing Up"
        }, 
        {
            "location": "/describing/#possible-tools-json-viewers", 
            "text": "jsoneditoronline.org  jsonviewer.stack.hu", 
            "title": "Possible Tools: JSON Viewers"
        }, 
        {
            "location": "/faq/", 
            "text": "The \nArchivers.space\n application is extremely fresh, we have some known issues and workarounds documented below.\n\n\n1) I'm looking at a URL, but I can't edit anything!\n\n\nMake sure you have clicked the big blue button \"\nCheckout this URL\n\" near the top. None of the fields can be edited until the URL is checked out.\n\n\n2) Why are URLs that may have already been archived by Ann Arbor available to research?\n\n\nWhen selecting a URL to review \"\n0\n\" is the \ndefault\n priority; it generally means that no-one has reviewed it \nUNLESS\n it says \nMAY HAVE BEEN HARVESTED AT ANN ARBOR\n.\n\n\nIn those cases, assign the priority to \"\n1\n\", so the URL drops down in the queue and then \nSKIP IT\n.\n\n\n3) What does it mean if it says \nCrawled by Internet Archive\n: Yes?\n\n\n\"\nCrawled by Internet Archive\n\" means the \npage itself\n was crawled; it may or may not mean the \ndataset\n was crawled.\n\n\nBased on what Heretrix \nCan and Can't Crawl\n, you will need to judge whether the dataset will be captured by the Internet Archive crawl and use your best judgement about whether to mark as \nDo not harvest\n.\n\n\n4) How should I handle overly broad sites with just a search form, e.g. noaa.gov?\n\n\nIn cases like \nnoaa.gov\n, you have to investigate  and try to find the data source a page is referencing and whether or not there is some way to query that data.\nIn many cases, it might be difficult or near impossible to isolate and query, depending on the kind of database.\n\n\nComplete the \nResearch\n section to the best of your abilities, especially the \nRecommended Approach for Harvesting Data\n.\n\n\n5) Do we have a scripting system set up preserving data or data endpoints that are updated regularly?\n\n\nNot yet; addressing these datasets is a goal going-forward.\n\n\nCurrently, indicate in the notes in both the \nResearch\n and \nHarvest\n sections that the dataset is updated regularly, and mark it complete anyway (note decision per @mattprice/this FAQ).\n\n\n6) What if I have a site and want to know if it has been crawled already?\n\n\nInternet Archive has both a \nWayback Machine Chrome Extension\n and \nAPIs\n you can use to check if something has been archived:\n\n\n\n\nWayback Machine Chrome Extension\n\n\nYou can also check on the Internet Archive site directly at \narchive.org/web/\n\n\nWayback CDX Server API\n\n\n\n\nThere is a \ncheck-ia\n script in the \nharvesting tools\n for batch URLs.\n\n\n7) What if the site has in fact been crawled well by the Internet Archive?\n\n\nIf the site includes only crawlable data, then there is no need to harvest it. These should be marked \nDo not harvest\n in the \nResearch\n phase.\n\n\nIf the site includes one of the forms of uncrawlable content:\n\n1) FTP\n\n2) Many Files\n\n3) Database\n\n4) Visualization/Interactive  \n\n\nThen mark accordingly in and harvest the datasets.\n\n\n8) What does it mean when it says \"checking out this url will unlock url: xxx\"?\n\n\nThat means you have another URL checked out. In order to avoid an overlap in efforts, when you check out a URL only you can work on it. By checking out a new URL the previous one will be unlocked.\n\n\n9) What do I do when there is stuff listed in the Link URL section?\n\n\nIf there are a bunch of sub-sites listed that are \nnot\n links, then you are on the master entry; the child entries are therefore just advisory and you should try to make sure that your harvesting includes all of the datasets contained across them, but otherwise keep going.\n\n\nIf the Link URL section has a single URL listed and it's a link, you are on a child item, which is the wrong place. Click the link and work on the master record.\n\n\n10) How do I partition large \"parent\" URLs (e.g., to reduce the size of the download \n 5 GB)?\n\n\nFrom the \noverview pane\n, click \nAdd Url\n on the top right side of app. Add a URL for each child and enter a description indicating these new URLs are children to the \"parent URL\". Make sure the priority of each child is the same as the parent.\n\n\nCheck out the parent URL, and under \nResearch\n use \"Link Url\" link it to all of its children and add a description. Make sure the priority of each child is the same as the parent. Start harvesting each child.\n\n\n11) Wifi is kind of Slow, are there workarounds for a faster connection?\n\n\n\n\n\n\nDo as much of the work as possible remotely: spin up a VM (e.g., AWS EC2, Digital Ocean droplet) or something, \nssh\n to those machines and do the downloading to there. The fewer people that are using the bandwidth onsite for big things, the less congestion this network will have.\n\n\n\n\n\n\nTether your phone :), thought if you do be mindful of bandwidth caps and don't forget to plug in your charger!\n\n\n\n\n\n\n12) Why can't I edit the harvesting section?\n\n\nArchivers is set up such that each URL moves through the stages of the workflow in sequence. In order to edit the \nHarvesting\n section, you will first need to mark \nResearch\n as complete. Look for the checkbox on the right-hand side at the top of the \nResearch\n section. Once you've checked it, make sure to hit \nSave\n.\n\n\n13) When harvesting, why doesn't clicking on the \nDownload Zipstarter\n button work?\n\n\nUnfortunately this is a known issue. Make sure you've marked \nResearch\n complete. Try reloading the page, or switching browsers if you can.\n\n\nThe App is not compatible with Safari.\n\n\n14) In the \nResearch\n section, what are all the checkboxes for?\n\n\nPlease read the DataRescue Workflow documentation for more info!\n\n\n15) I have a process improvement that would make this go better!\n\n\nGreat! Open an issue in your event's GitHub repository, or report it in the appropriate channel in your Slack team.\n\n\n16) How do I add a new event?\n\n\nAdmins can add events under the \"Events\" tab. Regular users will have to ask an admin for help!", 
            "title": "Archivers App FAQ"
        }, 
        {
            "location": "/faq/#1-im-looking-at-a-url-but-i-cant-edit-anything", 
            "text": "Make sure you have clicked the big blue button \" Checkout this URL \" near the top. None of the fields can be edited until the URL is checked out.", 
            "title": "1) I'm looking at a URL, but I can't edit anything!"
        }, 
        {
            "location": "/faq/#2-why-are-urls-that-may-have-already-been-archived-by-ann-arbor-available-to-research", 
            "text": "When selecting a URL to review \" 0 \" is the  default  priority; it generally means that no-one has reviewed it  UNLESS  it says  MAY HAVE BEEN HARVESTED AT ANN ARBOR .  In those cases, assign the priority to \" 1 \", so the URL drops down in the queue and then  SKIP IT .", 
            "title": "2) Why are URLs that may have already been archived by Ann Arbor available to research?"
        }, 
        {
            "location": "/faq/#3-what-does-it-mean-if-it-says-crawled-by-internet-archive-yes", 
            "text": "\" Crawled by Internet Archive \" means the  page itself  was crawled; it may or may not mean the  dataset  was crawled.  Based on what Heretrix  Can and Can't Crawl , you will need to judge whether the dataset will be captured by the Internet Archive crawl and use your best judgement about whether to mark as  Do not harvest .", 
            "title": "3) What does it mean if it says Crawled by Internet Archive: Yes?"
        }, 
        {
            "location": "/faq/#4-how-should-i-handle-overly-broad-sites-with-just-a-search-form-eg-noaagov", 
            "text": "In cases like  noaa.gov , you have to investigate  and try to find the data source a page is referencing and whether or not there is some way to query that data.\nIn many cases, it might be difficult or near impossible to isolate and query, depending on the kind of database.  Complete the  Research  section to the best of your abilities, especially the  Recommended Approach for Harvesting Data .", 
            "title": "4) How should I handle overly broad sites with just a search form, e.g. noaa.gov?"
        }, 
        {
            "location": "/faq/#5-do-we-have-a-scripting-system-set-up-preserving-data-or-data-endpoints-that-are-updated-regularly", 
            "text": "Not yet; addressing these datasets is a goal going-forward.  Currently, indicate in the notes in both the  Research  and  Harvest  sections that the dataset is updated regularly, and mark it complete anyway (note decision per @mattprice/this FAQ).", 
            "title": "5) Do we have a scripting system set up preserving data or data endpoints that are updated regularly?"
        }, 
        {
            "location": "/faq/#6-what-if-i-have-a-site-and-want-to-know-if-it-has-been-crawled-already", 
            "text": "Internet Archive has both a  Wayback Machine Chrome Extension  and  APIs  you can use to check if something has been archived:   Wayback Machine Chrome Extension  You can also check on the Internet Archive site directly at  archive.org/web/  Wayback CDX Server API   There is a  check-ia  script in the  harvesting tools  for batch URLs.", 
            "title": "6) What if I have a site and want to know if it has been crawled already?"
        }, 
        {
            "location": "/faq/#7-what-if-the-site-has-in-fact-been-crawled-well-by-the-internet-archive", 
            "text": "If the site includes only crawlable data, then there is no need to harvest it. These should be marked  Do not harvest  in the  Research  phase.  If the site includes one of the forms of uncrawlable content: \n1) FTP \n2) Many Files \n3) Database \n4) Visualization/Interactive    Then mark accordingly in and harvest the datasets.", 
            "title": "7) What if the site has in fact been crawled well by the Internet Archive?"
        }, 
        {
            "location": "/faq/#8-what-does-it-mean-when-it-says-checking-out-this-url-will-unlock-url-xxx", 
            "text": "That means you have another URL checked out. In order to avoid an overlap in efforts, when you check out a URL only you can work on it. By checking out a new URL the previous one will be unlocked.", 
            "title": "8) What does it mean when it says \"checking out this url will unlock url: xxx\"?"
        }, 
        {
            "location": "/faq/#9-what-do-i-do-when-there-is-stuff-listed-in-the-link-url-section", 
            "text": "If there are a bunch of sub-sites listed that are  not  links, then you are on the master entry; the child entries are therefore just advisory and you should try to make sure that your harvesting includes all of the datasets contained across them, but otherwise keep going.  If the Link URL section has a single URL listed and it's a link, you are on a child item, which is the wrong place. Click the link and work on the master record.", 
            "title": "9) What do I do when there is stuff listed in the Link URL section?"
        }, 
        {
            "location": "/faq/#10-how-do-i-partition-large-parent-urls-eg-to-reduce-the-size-of-the-download-5-gb", 
            "text": "From the  overview pane , click  Add Url  on the top right side of app. Add a URL for each child and enter a description indicating these new URLs are children to the \"parent URL\". Make sure the priority of each child is the same as the parent.  Check out the parent URL, and under  Research  use \"Link Url\" link it to all of its children and add a description. Make sure the priority of each child is the same as the parent. Start harvesting each child.", 
            "title": "10) How do I partition large \"parent\" URLs (e.g., to reduce the size of the download &lt; 5 GB)?"
        }, 
        {
            "location": "/faq/#11-wifi-is-kind-of-slow-are-there-workarounds-for-a-faster-connection", 
            "text": "Do as much of the work as possible remotely: spin up a VM (e.g., AWS EC2, Digital Ocean droplet) or something,  ssh  to those machines and do the downloading to there. The fewer people that are using the bandwidth onsite for big things, the less congestion this network will have.    Tether your phone :), thought if you do be mindful of bandwidth caps and don't forget to plug in your charger!", 
            "title": "11) Wifi is kind of Slow, are there workarounds for a faster connection?"
        }, 
        {
            "location": "/faq/#12-why-cant-i-edit-the-harvesting-section", 
            "text": "Archivers is set up such that each URL moves through the stages of the workflow in sequence. In order to edit the  Harvesting  section, you will first need to mark  Research  as complete. Look for the checkbox on the right-hand side at the top of the  Research  section. Once you've checked it, make sure to hit  Save .", 
            "title": "12) Why can't I edit the harvesting section?"
        }, 
        {
            "location": "/faq/#13-when-harvesting-why-doesnt-clicking-on-the-download-zipstarter-button-work", 
            "text": "Unfortunately this is a known issue. Make sure you've marked  Research  complete. Try reloading the page, or switching browsers if you can.  The App is not compatible with Safari.", 
            "title": "13) When harvesting, why doesn't clicking on the Download Zipstarter button work?"
        }, 
        {
            "location": "/faq/#14-in-the-research-section-what-are-all-the-checkboxes-for", 
            "text": "Please read the DataRescue Workflow documentation for more info!", 
            "title": "14) In the Research section, what are all the checkboxes for?"
        }, 
        {
            "location": "/faq/#15-i-have-a-process-improvement-that-would-make-this-go-better", 
            "text": "Great! Open an issue in your event's GitHub repository, or report it in the appropriate channel in your Slack team.", 
            "title": "15) I have a process improvement that would make this go better!"
        }, 
        {
            "location": "/faq/#16-how-do-i-add-a-new-event", 
            "text": "Admins can add events under the \"Events\" tab. Regular users will have to ask an admin for help!", 
            "title": "16) How do I add a new event?"
        }
    ]
}